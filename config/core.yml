# HomeClaw Core — single config (llama.cpp, local models, cloud models)
# LLM support: (1) Local models via llama.cpp server — OpenAI-compatible HTTP (one server per model, different ports).
#              (2) Cloud models via LiteLLM — same OpenAI-compatible API; set api_key_name and env var per provider.
# Main LLM and embedding can each be local or cloud independently (e.g. local chat + cloud embedding, or both local).
name: core
host: 0.0.0.0
port: 9000
mode: dev
# model_path: base path for local models (GGUF, etc.). Relative to project root; resolved on Windows, Mac, and Linux (use / in config; pathlib normalizes). Use ../models to put models outside the repo. Use ./models or leave empty for project_root/models.
model_path: ../models/
# When false: log [memory], [tools], [skills], [plugin], [TAM], [orchestrator] activity at INFO (verbose). When true: only INFO to console (less noise). On Windows CMD, use true to avoid huge output and "Press Space to continue" blocking.
silent: false
# When false: all logs go only to the log file (logs/core_debug.log); nothing to console. To monitor: "Get-Content logs\core_debug.log -Wait -Tail 50 -Encoding UTF8" (PowerShell; -Encoding UTF8 needed for Chinese/special chars on Windows) or "tail -f logs/core_debug.log" (Unix). When true: logs also go to console.
log_to_console: false
use_memory: true
# memory_backend: cognee (always default) | chroma. Both backends supported. Cognee: configure via cognee: or Cognee .env. Chroma: in-house RAG via core.yml database, vectorDB, graphDB.
# To empty memory for testing: POST or GET http://<core_host>:<core_port>/memory/reset (e.g. curl http://127.0.0.1:9000/memory/reset)
# To empty knowledge base (all users): POST or GET http://<core_host>:<core_port>/knowledge_base/reset
memory_backend: cognee   # default; set to chroma for in-house SQLite+Chroma
# memory_check_before_add: when true, for small/local models (≤8B or ≤14B+GPU) an extra LLM call asks "should we store?" and only adds to RAG memory if answer contains "yes". Default false = store every user message; retrieval (top-k, score) filters at read time. Set true only if you want to reduce write volume at the cost of one extra call per message.
# it will cost more time, suggest to be false.
memory_check_before_add: false
# default_location: optional fallback when no request/profile location (e.g. "New York, US"); see docs_design/SystemContextDateTimeAndLocation.md

# RAG memory summarization: summarize batches of old memories into long-term summaries; originals kept for keep_original_days then deleted; summaries kept forever. See docs_design/RAGMemorySummarizationDesign.md. Supported with Chroma and Cognee (default) when the backend exposes list/get_data/delete_data.
memory_summarization:
  enabled: false
  schedule: daily              # daily | weekly | next_run (next run time stored after each run)
  interval_days: 1              # for next_run: run again after this many days
  keep_original_days: 365       # TTL for raw memories; after this they are deleted (summary stays forever)
  min_age_days: 7               # only summarize memories older than this (days)
  max_memories_per_batch: 50     # max raw memories per summary batch
# Per-user profile (learned facts: name, birthday, preferences, families, etc.). One JSON file per user under profile.dir (default: database/profiles). See docs/UserProfileDesign.md.
profile:
  enabled: true
  dir: ""   # empty = database/profiles; or set a path for profile JSON files
# Optional: inject workspace bootstrap (IDENTITY.md, AGENTS.md, TOOLS.md) into system prompt. Default when missing: true.
use_workspace_bootstrap: true
# Which workspace dir to load (e.g. config/workspace_day vs config/workspace_night for day/night agents)
workspace_dir: config/workspace
# Root for file/folder tools and links (sandbox + share). Required for channel/companion file access. When empty, file tools return a clear error; do not use workspace for user files. Structure: homeclaw_root/{user_id}/ (private), homeclaw_root/share (shared), homeclaw_root/companion (companion app). Subfolders: output/, knowledgebase/.
homeclaw_root: "D:/homeclaw"
# Optional: inject curated long-term memory from a single Markdown file (AGENT_MEMORY.md). See docs_design/SessionAndDualMemoryDesign.md. Format: plain Markdown; recommended one fact per paragraph (see MemoryFilesUsage.md § Format).
# Agent memory file: default when missing = true, "", 20000. See docs_design/MemoryFilesUsage.md
use_agent_memory_file: true
agent_memory_path: ""   # empty = workspace_dir/AGENT_MEMORY.md; or set path relative to project or absolute
agent_memory_max_chars: 20000   # max chars to inject; 0 = no truncation
# Default when missing: use_agent_memory_search true, agent_memory_vector_collection homeclaw_agent_memory, bootstrap 20000/8000, use_daily_memory true, daily_memory_dir "".
# use_agent_memory_search: true (default) = inject capped bootstrap + index + tools; false = legacy inject last N chars.
#   false (legacy): Inject last agent_memory_max_chars of AGENT_MEMORY.md and yesterday+today daily files into the system prompt every time. Simpler but uses more tokens and can duplicate with RAG.
use_agent_memory_search: true
agent_memory_vector_collection: homeclaw_agent_memory   # Chroma collection for agent memory chunks
# Bootstrap cap (OpenClaw-style): when use_agent_memory_search is true, we inject a trimmed chunk of AGENT_MEMORY + daily so memory is always in context. When over cap we keep head (70%) + tail (20%) + marker.
agent_memory_bootstrap_max_chars: 20000       # default/cloud; total chars for agent + daily bootstrap block
agent_memory_bootstrap_max_chars_local: 8000 # when the request uses the local model (mix mode); use this smaller cap for small context windows
# Optional: inject short-term daily memory (memory/YYYY-MM-DD.md). Load yesterday + today on each request. Today's file is created automatically at startup/first use; the model can append via append_daily_memory tool. Format: plain Markdown; recommended one note per paragraph (see MemoryFilesUsage.md § Format).
use_daily_memory: true
daily_memory_dir: ""    # empty = workspace_dir/memory; or e.g. database/daily_memory
# Session management: pruning, lifecycle, API, dmScope. 
session:
  dm_scope: main              # main | per-peer | per-channel-peer | per-account-channel-peer
  # main (default): all DMs share one session for continuity. per-peer: isolate by sender id across channels.
  # per-channel-peer: isolate by channel + sender (recommended for multi-user inboxes).
  # per-account-channel-peer: isolate by account + channel + sender (multi-account inboxes).
  identity_links: {}          # optional: map canonical id -> list of provider-prefixed peer ids so same person shares DM across channels
  # e.g. { "alice": ["telegram:123", "discord:456"] } — then telegram:123 and discord:456 resolve to "alice" for session key
  prune_keep_last_n: 50       # after each turn (if prune_after_turn) or manually: keep last N turns per session
  prune_after_turn: false     # when true, run transcript prune after each reply (keeps last prune_keep_last_n)
  daily_reset_at_hour: -1     # -1 = disabled; 0-23 = new session when last activity was before today at this hour (local time)
  idle_minutes: -1            # -1 = disabled; >0 = new session when last activity older than N minutes
  api_enabled: true           # expose GET /api/sessions for plugin UIs
# When an unknown user (not in user.yml) tries to access, notify the owner via last-used channel so they can add to user.yml. See docs_design/OutboundMarkdownAndUnknownRequest.md.
notify_unknown_request: false
# Outbound reply: only when the result looks like Markdown, Core converts it before sending; otherwise original text is sent. whatsapp (default) = *bold* _italic_ ~strikethrough~ (most IMs); plain = strip to plaintext (use if channel doesn't support whatsapp format); none = no conversion. See docs_design/OutboundMarkdownAndUnknownRequest.md.
outbound_markdown_format: whatsapp
# Max concurrent LLM calls (channel queue + plugin API /api/plugins/llm/generate).
# Local model: use 1 (default) to avoid overloading one GPU/process.
# Cloud model: may use 2–10 (or per-provider limit) for parallel channel + plugin API; stay under provider RPM/TPM. See docs_design/PluginLLMAndQueueDesign.md.
llm_max_concurrent: 1
# Compaction: trim or summarize context when approaching model limit. Reduces token use and avoids overflows.
compaction:
  enabled: false              # when true, trim/summarize messages before LLM when over limit
  reserve_tokens: 4096        # tokens to leave free for the model's reply (not the same as ctx_size). Should be less than llama_cpp.ctx_size (e.g. 1024–4096); ctx_size is total window, this is just the reserve for output.
  max_messages_before_compact: 30   # max user+assistant+tool messages to keep (older trimmed or summarized)
  compact_tool_results: true  # when true, trim or replace large tool results in the tool loop with a short placeholder when over limit
  # Single flag: when true, memory is written only in a dedicated flush turn before compaction; main prompt does not ask the model to call append_*. Omit or set true for default; set false to use the old behavior (model writes during the turn).
  memory_flush_primary: true
  memory_flush_prompt: "Pre-compaction memory flush. From the conversation so far, store durable memories now. Use append_agent_memory for lasting facts and append_daily_memory for today's short-term notes. APPEND only; do not overwrite. If nothing to store, reply briefly with 'Nothing to store.'"
# Optional: enable tool layer (tool registry; model can call tools by name with args); see Design.md §3.6
use_tools: true
# Optional: inject skills (SKILL.md) from skills_dir into system prompt; see Design.md §3.6
use_skills: true
skills_dir: config/skills
# Optional extra dirs for skills (paths relative to project root). User can put more skill folders here; merged with skills_dir (first wins by folder name). See docs_design/SkillsAndPluginsSamePolicy.md.
# skills_extra_dirs: [config/external_skills]
# Folder names to not load (case-insensitive). Use to disable a skill without removing it (e.g. ["x-api-1.0.0"]).
# skills_disabled: []
# Skills: skills_use_vector_search=false → include ALL skills. true → RAG (skills_max_retrieved → threshold → skills_max_in_prompt). Same for plugins.

# Max chars per plugin description in the routing block. 0 = no truncation (default). With RAG (plugins_use_vector_search) or plugins_max_in_prompt we already limit how many plugins appear; this only caps per-description length. Use 0 for full descriptions; set 512 or 300 only to shrink prompt or cap one very long description.
plugins_description_max_chars: 0
# Default: load all skills/plugins; no vector store. If skills_use_vector_search or plugins_use_vector_search (or related keys) are omitted, we use this default: load all, do not create or use the vector store.
# Vector retrieval for skills: when false or omitted → include ALL skills; when true → create store, sync, and RAG (skills_max_retrieved → threshold → skills_max_in_prompt). See docs_design/SkillsAndPluginsSamePolicy.md.
skills_use_vector_search: false
skills_vector_collection: homeclaw_skills
skills_max_retrieved: 10      # when using vector search: max skills to retrieve from RAG (then threshold + skills_max_in_prompt cap)
skills_max_in_prompt: 5     # when skills_use_vector_search=true: cap; when false, all skills are included (this value ignored)
# Min similarity (0..1) to keep a skill; below this, RAG drops it. 0.5 can drop cross-lingual or short queries; 0.3 or 0.0 improves selection (e.g. Chinese "创建图片" vs English skill text). Use 0.3 so Chinese/mixed queries (e.g. 发一条X, 生成HTML slides) still retrieve relevant skills. Force-include rules still add skills by pattern.
skills_similarity_threshold: 0.5
skills_refresh_on_startup: true
# Optional test folder: full sync every time (id = test__<folder>). Production skills_dir can be incremental (only new).
skills_test_dir: ""
skills_incremental_sync: false
# Which skills get body in the prompt (SKILL.md + USAGE.md prefix). For these folders only, so the model can answer "how do I use this?". Set skills_include_body_max_chars > 0 (e.g. 8000) to cap body length and avoid exceeding context.
skills_include_body_for: [maton-api-gateway-1.0.0, x-api-1.0.0, meta-social-1.0.0]
# skills_include_body_max_chars: 8000   # 0 = no truncation; when > 0, cap body for skills_include_body_for to this many chars
# Optional: force-include rules add an extra instruction when the query matches. With include-all you do NOT need these—LLM selects from the full list. Prefer trigger in each skill's SKILL.md. Uncomment and edit below if you want a global override.
# Maton: one skill provides 100+ services (Slack, LinkedIn, Outlook, HubSpot, Notion, Gmail, Stripe, etc.). Force-include when user mentions any of them so the model uses run_skill(maton-api-gateway-1.0.0, request.py) instead of replying without calling.
skills_force_include_rules:
  - patterns:
    - "slack|send.*slack|slack.*(message|channel)"
    - "linkedin|领英|发.*linkedin|post.*linkedin|发送.*linkedin"
    - "outlook|(outlook|microsoft).*(email|mail|calendar|contact)"
    - "hubspot|(hubspot).*(contact|deal|company)"
    - "notion|(notion).*(database|page|block)"
    - "gmail|google.*mail|(gmail).*(send|list|search)"
    - "stripe|(stripe).*(customer|payment|charge|invoice)"
    - "google.*calendar|calendar.*event|(google).*(calendar)"
    - "google.*(sheet|spreadsheet)|(sheet|spreadsheet).*row"
    - "salesforce|(salesforce).*(contact|opportunity|soql)"
    - "airtable|(airtable).*(base|record|table)"
    - "calendly|(calendly).*(event|availability|schedule)"
    - "github|(github).*(repo|issue|pr|pull.*request)"
    - "maton|api gateway|gateway\\.maton|maton\\.ai"
    folders: [maton-api-gateway-1.0.0]
    instruction: "User asked to use an external service (Slack, LinkedIn, Outlook, HubSpot, Notion, Gmail, Stripe, Google Calendar/Sheets, Salesforce, Airtable, Calendly, GitHub, etc.). Use run_skill(skill_name='maton-api-gateway-1.0.0', script='request.py') with app and path from the maton skill body (Supported Services table and references/). Do not claim the action was done without calling the skill. For LinkedIn post: GET linkedin/rest/me then POST linkedin/rest/posts with commentary."
# Example (uncomment to add more):
#   - patterns: ["乔布斯|极简.*演示|HTML\\s*slides|html\\s*slides|幻灯片|生成.*HTML"]
#     folders: [html-slides-1.0.0]
#     instruction: "User asked for 乔布斯-style or HTML slides. You have the html-slides skill: follow its steps, then file_write or save_result_page to output/ and return the link. Do not say you have no skill."
#   - patterns: ["generate.*image|create.*image|画.*图|生成.*图|来一张图|做.*图|创建图片"]
#     folders: [image-generation-1.0.0]
#     instruction: "User asked to generate or create an image. Call run_skill(skill_name='image-generation-1.0.0', script='generate_image.py', args=['--prompt', '<description>']). Do not say no image tool."
#   - patterns: ["发.*[Xx]|发推|发一条.*[Xx]|post.*tweet|tweet.*post"]
#     folders: [x-api-1.0.0, social-media-agent-1.0.0]
#     instruction: "User asked to post to X/Twitter. Use x-api-1.0.0 (run_skill with request.py) if you have X_ACCESS_TOKEN, else social-media-agent-1.0.0 (browser). Do not say you have no skill."
#   - patterns: ["summarize|总结|摘要|概括|summarise"]
#     folders: [summarize-1.0.0]
#     instruction: "User asked to summarize. Use run_skill(skill_name='summarize-1.0.0', ...) or the summarize CLI / web_search as needed. Do not say you have no skill."
# Vector retrieval for plugins: when false or omitted → include ALL plugins; when true → create store, sync, and RAG. Same default as skills.
plugins_use_vector_search: false
plugins_vector_collection: homeclaw_plugins
plugins_max_retrieved: 10     # when using vector search: max plugins to retrieve from RAG
plugins_max_in_prompt: 15    # when plugins_use_vector_search=true: cap; when false, all plugins included (this value ignored)
plugins_similarity_threshold: 0.0
plugins_refresh_on_startup: true
# Optional: when the query matches a pattern, add an instruction (and ensure plugin in block when using plugin RAG). With include-all you do NOT need these—LLM selects from full list. Uncomment and edit below if you want an override.
plugins_force_include_rules: []
# Example (uncomment to use):
# plugins_force_include_rules:
#   - pattern: "open\\s+(the\\s+)?browser|navigate\\s+to|go\\s+to\\s+https?://"
#     plugins: [homeclaw-browser]
#     instruction: "The user asked to open a URL or use the browser. Use route_to_plugin(plugin_id='homeclaw-browser', capability_id='browser_navigate', parameters={\"url\": \"<URL>\"})."
#   - pattern: "生成.*PPT|做个PPT|做.*PPT|生成.*演示|create.*ppt|make.*presentation"
#     plugins: [ppt-generation]
#     instruction: "The user asked to create a PPT or PowerPoint presentation. Use route_to_plugin(plugin_id='ppt-generation', capability_id='create_from_outline' or 'create_from_source', parameters with outline/source and optional title). The plugin saves to the user's output folder and returns a link; include that link in your reply so the user can open the file."
# Optional extra dirs to scan for manifest-based external plugins (http/subprocess/mcp). Paths relative to project root. Drop a folder with plugin.yaml (type: http|subprocess|mcp) and restart Core to discover it. See docs_design/SkillsAndPluginsSamePolicy.md.
# plugins_extra_dirs: [config/external_plugins]
# System plugins (system_plugins/): when true, Core starts each plugin (e.g. node server.js) and runs register (e.g. node register.js) so one command runs Core + all system plugins. See system_plugins/README.md.
system_plugins_auto_start: true   # set true to start homeclaw-browser etc. with Core
system_plugins: [homeclaw-browser]                 # optional allowlist by plugin id (folder name); empty = start all discovered
system_plugins_env:
  homeclaw-browser:
    BROWSER_HEADLESS: false
# Per-plugin env vars (plugin id = folder name under system_plugins/). Each plugin gets only its own env.
# system_plugins_env:
#   homeclaw-browser:
#     BROWSER_HEADLESS: "false"
#   other-plugin:
#     FOO: "bar"
# Orchestrator + TAM + plugins are always enabled. Single choice: routing style.
# orchestrator_unified_with_tools: true (default) — main LLM with tools does routing (route_to_tam / route_to_plugin). One LLM call.
# orchestrator_unified_with_tools: false — separate orchestrator LLM call for intent + plugin selection, then plugin runs.
# orchestrator_unified_with_tools: true
# Timeout for orchestrator intent/plugin LLM call and plugin.run() when unified is false. Unit: seconds. 0 = no timeout.
# orchestrator_timeout_seconds: 60
# Recommended max time (seconds) for clients/proxies waiting for Core to respond (e.g. /inbound). 0 = unlimited (no recommended limit). Long tasks (document_read, summarization, video) may need several minutes; set >0 (e.g. 300) if you want a cap, or 0 so clients/proxies can wait as long as needed. Proxies (nginx, Cloudflare) should set read_timeout >= this when >0.
# inbound_request_timeout_seconds: 0
# Prompt manager: load prompts from config/prompts with language/model overrides. See docs/PromptManagement.md.
use_prompt_manager: true
prompts_dir: config/prompts
prompt_default_language: en
prompt_cache_ttl_seconds: 0   # Unit: seconds. 0 = cache by file mtime; >0 = TTL
# Optional: require API key for /inbound and /ws when exposing Core on the internet; see docs/RemoteAccess.md
auth_enabled: true
auth_api_key: "1234567890"   # when auth_enabled: true, require X-API-Key header or Authorization: Bearer <key>. Also used to sign file/report links (GET /files/out) when core_public_url is set.
# Public URL that reaches this Core instance (e.g. https://homeclaw.example.com via Cloudflare Tunnel). Used for: (1) file/report links core_public_url/files/out?path=...&token=... (2) GET /pinggy scan-to-connect page and QR for Companion. Leave empty for local-only or if using pinggy.token for tunnel.
core_public_url: "https://homeclaw.gpt4people.online"
# Pinggy tunnel: when token is set, Core starts a tunnel to this port and serves /pinggy with public URL + QR for Companion. open_browser: true = open browser to /pinggy when tunnel is ready. See docs_design/PinggyIntegration.md.
pinggy:
  token: ""        # empty = do not start Pinggy; set to your Pinggy token to enable
  open_browser: true   # when true, open default browser to http://127.0.0.1:<port>/pinggy when tunnel is ready
# Tool layer config (used when use_tools: true). Cross-platform (Mac/Linux/Windows) where possible.
# Built-in tools: time, cron_*, sessions_*, session_status; memory_search, memory_get; file_*,
# folder_list; fetch_url, web_search; browser_*; exec, process_*; image; channel_send; run_skill;
# models_list, agents_list; webhook_trigger; env, cwd, platform_info, echo.
# The keys below are the only ones read by the tool layer; other tools have no configurable options.
tools:
  # exec: allowed command names. Empty or omit = platform default (Unix: ls, cat, pwd; Windows: dir, type, cd). Set explicitly to override.
  exec_allowlist: []   # e.g. [ "date", "whoami", "echo", "pwd", "ls", "cat" ] on Unix; [ "date", "whoami", "echo", "dir", "type", "cd" ] on Windows
  exec_timeout: 30   # Unit: seconds
  # homeclaw_root is at root of core.yml (not here). When set: under it (1) share folder (file_read_shared_dir, default "share"); (2) per-user folders; (3) "companion" folder. When homeclaw_root empty = workspace_dir.
  # Name of shared subfolder under homeclaw_root (all users and companion can use "share/..." paths). Default "share".
  # file_read_shared_dir: "share"
  # Reserved subfolder for user/companion generated files (reports, images, exports). Path "output/..." → base/{user_id}/output/ or base/companion/output/. See docs_design/FileSandboxDesign.md.
  # file_read_output_dir: "output"
  # Max characters returned by file_read when the tool does not pass max_chars (default 32000). Increase for long PDFs/documents (e.g. 128000).
  file_read_max_chars: 5000000
  # Max size (KB) for save_result_page generated HTML; default 500. Used when writing report to user output folder.
  # save_result_page_max_file_size_kb: 500
  # (Baidu API key is set in the skill: config/skills/baidu-search-1.1.0/config.yml api_key, or env BAIDU_API_KEY)
  run_skill_allowlist: []   # run_skill: if set, only these script names under skill/scripts/. Use [] to allow all scripts from skills. .sh on Windows runs via bash/WSL if available.
  run_skill_py_in_process_skills: []   # run_skill: .py scripts run in subprocess by default. List skill folder names here (e.g. ["image-generation-1.0.0"]) to run those skills' .py in Core's process.
  # run_skill_auto_install_missing: { "google": "google-genai pillow" }   # optional: on ModuleNotFoundError, pip install and retry once (only when using subprocess)
  run_skill_timeout: 300   # Unit: seconds
  run_plugin_in_process_plugins: []   # Built-in plugins run in subprocess by default (never crash Core). List plugin_ids here (e.g. ["headlines", "quotes"]) to run those in Core's process when needed.
  run_plugin_timeout: 300   # Plugin subprocess timeout (seconds)
  web:                  # web_search: free (no key) duckduckgo | free tier google_cse (100/day), bing (1000/mo) | tavily, brave, serpapi. Browser: web_search_browser (Google/Bing/Baidu, no key, needs Playwright).
    search:
      provider: tavily  # duckduckgo (no key) | google_cse (100 free/day) | bing (1000 free/mo) | tavily | brave | serpapi. When tavily: Tavily is used first (key under tavily below).
      duckduckgo: {}    # provider=duckduckgo: no API key; pip install duckduckgo-search
      google_cse:       # provider=google_cse: 100 free queries/day
        api_key: ""     # or GOOGLE_CSE_API_KEY
        cx: ""          # Search Engine ID from https://programmablesearchengine.google.com/ (or GOOGLE_CSE_CX)
      bing:             # provider=bing: 1000 free transactions/month (APIs retiring Aug 2025)
        api_key: ""     # or BING_SEARCH_SUBSCRIPTION_KEY (Azure)
      brave:            # when provider=brave
        api_key: ""     # or set BRAVE_API_KEY
        search_type: web   # web (default) | news | video | image
      serpapi:          # when provider=serpapi (paid; ~250 free/mo)
        api_key: ""     # or set SERPAPI_API_KEY
        engine: google # google (default) | bing | baidu
      tavily:          # default provider (free tier: 1000 searches/month). Same key used for search, extract, crawl, research.
        api_key: "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"    # Set here (e.g. tvly-xxx) OR set env TAVILY_API_KEY where Core runs. Get key: https://tavily.com
        search_depth: basic   # search only: basic | fast | advanced | ultra-fast (affects latency vs relevance)
        topic: general        # search only: general | news | finance
        time_range: ""        # search only: day | week | month | year (empty = no filter)
      # When no API key or primary fails (expired/rate limit), use DuckDuckGo (no key). Requires: pip install duckduckgo-search
      fallback_no_key: true   # true = try DuckDuckGo when Tavily/Brave unavailable
      fallback_max_results: 5 # max results from fallback (3–10)
  # --- Browser tools (Core vs external plugin) ---
  # browser_enabled: true  — Core registers built-in browser tools (browser_navigate, browser_snapshot, browser_click, browser_type) when Playwright is installed. The LLM can call these tools directly.
  # browser_enabled: false — Core does NOT register browser tools. Use this when you want browser automation via the system plugin homeclaw-browser (Node.js): register that plugin and set browser_enabled false so the LLM routes all browser actions through route_to_plugin(homeclaw-browser, browser_navigate, ...). Avoids duplicate tools and lets you use the Node.js plugin for canvas/nodes. See system_plugins/homeclaw-browser/README.md.
  browser_enabled: false
  # When browser_enabled true and Playwright available: set to false to show the browser window (local testing only; on servers without a display use true or run under Xvfb).
  browser_headless: false
  # Per-tool execution timeout. Unit: seconds. Prevents a single tool from hanging the system. 0 = no timeout. For PDF read + summarize, use 300–600 so document_read and LLM reply can finish.
  tool_timeout_seconds: 0
# Optional: user knowledge base (documents, web search, URLs, manual). See docs/MemoryAndDatabase.md.
# backend: auto (default) = use same as memory_backend (cognee -> Cognee DB/vector, chroma -> core.yml vectorDB).
# Set to "cognee" or "chroma" to override (e.g. Cognee memory + built-in RAG for KB).
knowledge_base:
  enabled: true
  backend: auto          # auto | cognee | chroma
  collection_name: homeclaw_kb   # used only when backend is chroma
  chunk_size: 800
  chunk_overlap: 100
  unused_ttl_days: 90   # Unit: days. Cognee: remove sources older than this (age-based); chroma: remove by last_used_timestamp
  max_sources_per_user: 0   # Cognee only: max KB sources per user (0 = no cap). Oldest evicted when over cap.
  embed_timeout: 600   # Unit: seconds
  store_timeout: 600   # Unit: seconds
  # Configurable similarity threshold (0-1, higher = more relevant). Only inject chunks with score >= this; if none pass, no KB block. Set to null to disable (inject top-k regardless).
  retrieval_min_score: 0.5   # e.g. 0.5 or 0.7
  # Per-user folder sync: scan {homeclaw_root}/{user_id}/knowledgebase/, add new/changed files to KB, remove when file deleted. See docs_design/PerUserKnowledgeBaseFolder.md.
  folder_sync:
    enabled: false
    folder_name: knowledgebase   # subdir under each user's sandbox
    schedule: ""   # cron expression, e.g. "0 */6 * * *" every 6h; empty = only on-demand via POST /knowledge_base/sync_folder
    allowed_extensions: [".md", ".txt", ".pdf", ".docx", ".html", ".htm", ".rst", ".csv", ".ppt", ".pptx"]
    max_file_size_bytes: 5000000
    resync_on_mtime_change: true

# File-understanding (request.files from channels): always inject short notice with paths so the model uses document_read / knowledge_base_add. When user sends file(s) only (no text) and doc is not too big, add to KB directly. See docs_design/FileUnderstandingDesign.md.
file_understanding:
  # When user sends only file(s) (no or negligible text): add extracted document to the user's KB only if extracted text length <= this (chars). If larger, skip; user can say what they want and model uses document_read / knowledge_base_add. 0 = never auto-add (tool-based only).
  add_to_kb_max_chars: 5000000   # e.g. 50000; 0 = never add to KB on file-only

# ——— 1. llama.cpp server settings (for local models) ———
# ctx_size, predict, temp are used when starting the llama.cpp server.
# For chat completions, max_tokens and temperature are sent per request (see completion below).
# ctx_size: total context window (input + output). Increase if you see truncation, "context length exceeded",
#   or poor routing when using many plugins/skills (full descriptions), long chat history, or large tool lists.
# Typical values: 32768 (32K, default), 65536 (64K), 131072 (128K). Higher = more VRAM/RAM and may slow inference; check your model's max context.
llama_cpp:
  ctx_size: 32768
  predict: 8192
  temp: 0.7
  threads: 8
  n_gpu_layers: 99
  verbose: false
  repeat_penalty: 1.5
  chat_format: null
  function_calling: true

  embedding:
    ctx_size: 0      # 0 = use model default (recommended; avoids forcing 8192 on models that support less)
    threads: 4
    n_gpu_layers: 20
  
  # Optional: overrides for the embedding server only (local embedding model). Merged over the keys above.
  # Use when the embedding model needs different ctx_size/threads/n_gpu_layers. ctx_size 0 = use model's native n_ctx.
  # embedding:
  #   ctx_size: 0      # 0 = use model default (recommended; avoids forcing 8192 on models that support less)
  #   threads: 8
  #   n_gpu_layers: 99

# ——— Completion (per-request generation) ———
# All parameters below are sent with every chat completion call. Omit or comment out to use server default or llama_cpp fallback where noted.
# Fallbacks: max_tokens → llama_cpp.predict; temperature/top_p/repeat_penalty → llama_cpp.temp / (none) / llama_cpp.repeat_penalty.
completion:
  max_tokens: 8192          # max tokens to generate per response (fallback: llama_cpp.predict)
  temperature: 0.7          # sampling temperature 0..2 (fallback: llama_cpp.temp)
  top_p: 1.0                # nucleus sampling 0..1
  presence_penalty: 0       # OpenAI-style; penalize tokens that appear in text so far
  frequency_penalty: 0      # OpenAI-style; penalize by token frequency
  repeat_penalty: 1.5       # llama.cpp-style; sent in extra_body (fallback: llama_cpp.repeat_penalty)
  seed: null                 # optional; int for reproducible sampling
  stop: null                 # optional; list of strings, e.g. ["</s>", "Human:"]
  # logit_bias: {}          # optional; dict of token id to bias (-100..100)
  # n: 1                    # number of completions (default 1)
  # response_format: null   # optional; e.g. { "type": "json_object" }
  # timeout: null            # optional; Unit: seconds
  image_max_dimension: 512   # 0 = no resize. Set e.g. 1024 to resize images before sending to vision model (keeps aspect ratio; requires Pillow)

# ——— 2. Local models (llama.cpp server per model; path relative to model_path) ———
# Optional per model: mmproj (vision), lora (adapter(s)), lora_base (optional base for LoRA).
#   mmproj: path to projector .gguf; llama-server -m <path> --mmproj <mmproj>
#   lora: single path or array of paths to LoRA adapter .gguf; each gets --lora <path> (multi-LoRA if supported)
#   lora_base: optional; only when the adapter requires a specific base; --lora-base <path>
# Optional supported_media: list of media types the model can handle — [image], [image, audio], or [image, audio, video].
#   If omitted: local model defaults to [] (text-only) unless mmproj is set, then [image]. Cloud model defaults to [image, audio, video].
#   Use supported_media to override (e.g. restrict cloud to [image] or enable image for a local model without mmproj).
# Paths are relative to model_path (same as path).
local_models:
  - id: embedding_text_model
    alias: embedding
    path: Qwen3-Embedding-0.6B-Q8_0.gguf  #Qwen3-Embedding-4B-Q4_K_M.gguf,  bge-m3-Q5_K_M.gguf
    host: 127.0.0.1
    port: 5066
    capabilities: [embedding]
  # Example vision model (uncomment and set paths): mmproj required; supported_media defaults to [image]
  # - id: llava_1_5_7b
  #   alias: LLaVA 1.5 7B
  #   path: llava/llava-v1.5-7b-model.gguf
  #   mmproj: llava/llava-v1.5-7b-mmproj.gguf
  #   # supported_media: [image]   # optional; default [image] when mmproj is set
  #   host: 127.0.0.1
  #   port: 5020
  #   capabilities: [Chat]
  # Example LoRA (uncomment): lora (single path or array), lora_base optional
  # - id: base_with_lora
  #   alias: Base + LoRA
  #   path: base/base-model.gguf
  #   lora: adapters/adapter-01.gguf
  #   # lora_base: base/base-model.gguf   # optional; only if adapter needs a specific base
  #   host: 127.0.0.1
  #   port: 5021
  #   capabilities: [Chat]
  # Multi-LoRA (if llama-server supports multiple --lora): use array
  #   lora: [adapters/a1.gguf, adapters/a2.gguf]
  - id: main_vl_model_4B
    alias: main_vl_model_4B
    path: Qwen3VL-4B-Instruct-Q4_K_M.gguf 
    mmproj: mmproj-Qwen3VL-4B-Instruct-F16.gguf
    host: 127.0.0.1
    port: 5023
    capabilities: [Chat]
    supported_media: [image]

  - id: main_vl_model_8B
    alias: main_vl_model_8B
    path: Qwen3-VL-8B-Instruct-Q4_K_M.gguf
    mmproj: mmproj-Qwen3-VL-8B-BF16.gguf
    host: 127.0.0.1
    port: 5024
    capabilities: [Chat]
    supported_media: [image]

  - id: gemma_3_4b_it_4B
    alias: gemma_3_4b_it_4B
    path: gemma-3-4b-it-Q4_K_M.gguf
    mmproj: mmproj-gemma-3-4b-BF16.gguf
    host: 127.0.0.1
    port: 5025
    capabilities: [Chat]
    supported_media: [image]

  # Optional: Layer 3 classifier for mix mode (hybrid_router.slm.model). Small model (e.g. Qwen3-0.5B) on a separate port; Core starts it when main_llm_mode: mix and slm.enabled.
  - id: classifier_0_6b
    alias: classifier
    path: Qwen3-0.6B-Q5_K_M.gguf
    host: 127.0.0.1
    port: 5089
    capabilities: [Chat]


# ——— 3. Cloud models (LiteLLM; one service per entry, each host/port) ———
# api_key_name: env var name (e.g. GEMINI_API_KEY); Core uses that env var for the key. Optional api_key: set the key here instead of env (use for convenience only; avoid committing secrets).
# Start LiteLLM per model or use one proxy. See https://github.com/BerriAI/litellm and docs.litellm.ai/docs/providers
# Use as main LLM (Chat/Functioncall) or as embedding model (capabilities: [embedding]); main and embedding can mix local + cloud.
# Optional supported_media per entry: [image], [image, audio], or [image, audio, video]. If omitted, cloud models default to [image, audio, video].
cloud_models:
  # OpenAI (chat and embedding). supported_media: gpt-4o supports image only; gpt-4o-audio-preview supports audio only (see MediaSupportAndProviders.md).
  # - id: OpenAI-GPT4o
  #   alias: OpenAI GPT-4o
  #   path: openai/gpt-4o
  #   host: 127.0.0.1
  #   port: 4001
  #   api_key_name: OPENAI_API_KEY
  #   supported_media: [image]   # gpt-4o = vision only; audio is a different model (gpt-4o-audio-preview)
  #   capabilities: [Functioncall, Chat]
  # - id: OpenAI-GPT4o-mini
  #   alias: OpenAI GPT-4o mini
  #   path: openai/gpt-4o-mini
  #   host: 127.0.0.1
  #   port: 4001
  #   api_key_name: OPENAI_API_KEY
  #   # supported_media: [image, audio, video]   # optional; cloud default is all
  #   capabilities: [Functioncall, Chat]
  # Google Gemini
  # - id: Gemini-2.5-Flash
  #   alias: Gemini 2.5 Flash
  #   path: gemini/gemini-2.5-flash
  #   host: 127.0.0.1
  #   port: 4002
  #   api_key_name: GEMINI_API_KEY
  #   api_key: 
  #   capabilities: [Functioncall, Chat]
  # Anthropic Claude
  # - id: Claude-Sonnet
  #   alias: Claude 3.5 Sonnet
  #   path: anthropic/claude-3-5-sonnet-20241022
  #   host: 127.0.0.1
  #   port: 4003
  #   api_key_name: ANTHROPIC_API_KEY
  #   capabilities: [Functioncall, Chat]
  # # Ollama (local; no API key)
  # - id: Ollama-qwen2
  #   alias: Ollama Qwen2
  #   path: ollama/qwen2.5:7b
  #   host: 127.0.0.1
  #   port: 4004
  #   capabilities: [Chat]
  # DeepSeek
  - id: DeepSeek-Chat
    alias: DeepSeek Chat
    path: deepseek/deepseek-chat
    host: 127.0.0.1
    port: 4005
    api_key_name: DEEPSEEK_API_KEY
    api_key: sk-eb1f151236524086a0967357233b9c88
    capabilities: [Chat]
  # Alibaba Qwen (Dashscope)
  # - id: Qwen-Turbo
  #   alias: Qwen Turbo (Dashscope)
  #   path: dashscope/qwen-turbo
  #   host: 127.0.0.1
  #   port: 4006
  #   api_key_name: DASHSCOPE_API_KEY
  #   capabilities: [Chat]
  # # Groq
  # - id: Groq-Llama70B
  #   alias: Groq Llama 3.1 70B
  #   path: groq/llama-3.1-70b-versatile
  #   host: 127.0.0.1
  #   port: 4007
  #   api_key_name: GROQ_API_KEY
  #   capabilities: [Chat]
  # # Mistral
  # - id: Mistral-Large
  #   alias: Mistral Large
  #   path: mistral/mistral-large-latest
  #   host: 127.0.0.1
  #   port: 4008
  #   api_key_name: MISTRAL_API_KEY
  #   capabilities: [Functioncall, Chat]
  # # xAI (Grok)
  # - id: xAI-Grok
  #   alias: xAI Grok 2
  #   path: xai/grok-2
  #   host: 127.0.0.1
  #   port: 4009
  #   api_key_name: XAI_API_KEY
  #   capabilities: [Chat]
  # # OpenRouter (many models via one API)
  # - id: OpenRouter-GPT4o
  #   alias: OpenRouter GPT-4o
  #   path: openrouter/openai/gpt-4o
  #   host: 127.0.0.1
  #   port: 4010
  #   api_key_name: OPENROUTER_API_KEY
  #   capabilities: [Functioncall, Chat]
  # # Cohere
  # - id: Cohere-CommandR
  #   alias: Cohere Command R+
  #   path: cohere/command-r-plus
  #   host: 127.0.0.1
  #   port: 4011
  #   api_key_name: COHERE_API_KEY
  #   capabilities: [Chat]
  # # Perplexity (search-augmented)
  # - id: Perplexity-Sonar
  #   alias: Perplexity Sonar
  #   path: perplexity/llama-3.1-sonar-small-128k-online
  #   host: 127.0.0.1
  #   port: 4012
  #   api_key_name: PERPLEXITY_API_KEY
  #   capabilities: [Chat]

# ——— Media support (image / audio / video) for main_llm ———
# See docs_design/MediaSupportAndProviders.md for how LiteLLM, OpenAI, and Gemini support image/audio/video.
# WHERE TO SET: Add optional "supported_media" on the model entry that you use as main_llm (that entry is under
#   local_models: or cloud_models: below; main_llm points to it, e.g. main_llm: cloud_models/OpenAI-GPT4o).
# CLOUD: We do not call the API to discover capabilities. We default cloud to [image, audio, video]; for OpenAI
#   you must set supported_media per model: gpt-4o supports image only → supported_media: [image]; gpt-4o-audio-preview
#   supports audio only → supported_media: [audio]. Gemini (e.g. 1.5 Pro) often supports all → [image, audio, video].
# LOCAL: Default [] (text-only); if the model has mmproj (vision), default [image]. Override with supported_media if needed.
# HOW IT WORKS: Core calls main_llm_supported_media() and only sends image/audio/video parts in that list; others
#   are omitted and a short note is added so the model does not crash.

# ——— Selected main and embedding model: local_models/<id> or cloud_models/<id> ———
# main_llm: optional when main_llm_mode is set. When mode is "local" we use main_llm_local; when "cloud" we use main_llm_cloud; when "mix" we use both per request. Set main_llm only if you omit main_llm_mode (legacy: derive mode from main_llm).
#main_llm: local_models/main_vl_model_4B
main_llm_mode: mix
main_llm_local: local_models/main_vl_model_8B
main_llm_cloud: cloud_models/DeepSeek-Chat
# --- Mix mode routing thresholds (brief) ---
# Heuristic (Layer 1): no threshold. When enabled, first match (long-input or keyword rule) wins.
# Semantic (Layer 2): score = embedding similarity to the chosen route (0..1). Only the best-matching route is considered; if its similarity_score >= threshold, that route is chosen. Higher threshold = need stronger similarity (stricter; more fall-through to next layer). Lower = accept weaker matches.
# SLM (Layer 3): classifier mode returns a definitive Local or Cloud—no threshold. Perplexity mode uses perplexity_threshold (see below).
# Perplexity (Layer 3, when slm.mode: perplexity): score = average log-probability of probe tokens (negative, e.g. -0.2 to -2.0). avg_logprob >= perplexity_threshold → local; < threshold → cloud. Higher threshold (e.g. -0.3) = need higher confidence to stay local (more go cloud). Lower (e.g. -0.8) = easier to stay local.
hybrid_router:
  # default_route is used when no layer picks a route: no heuristic/semantic match, and (if prefer_cloud_if_long_chars not hit) Layer 3 returns nothing or is skipped. Also used when prefer_cloud_if_long_chars is set and query is long (then we skip Layer 3 and use default_route).
  default_route: local
  show_route_in_response: true
  # If the chosen model (e.g. cloud) fails (timeout/error/empty): retry once with the other route (e.g. local). If both fail, user sees "Sorry, something went wrong...".
  fallback_on_llm_error: true
  # If set (e.g. 800): when no route from heuristic/semantic and query length > this, use default_route and skip perplexity probe (avoids local model overconfidence on long/complex prompts).
  prefer_cloud_if_long_chars: 800
  heuristic:
    enabled: true
    rules_path: config/hybrid/heuristic_rules.yml
  semantic:
    enabled: true
    threshold: 0.6   # min similarity (0..1) to accept the best route; higher = stricter, more fall-through
    routes_path: config/hybrid/semantic_routes.yml
  # Layer 3: classifier (small model) or perplexity (main model confidence probe).
  # - mode: classifier = small model answers "Local or Cloud?" (uses model above).
  # - mode: perplexity = main local model generates a few tokens with logprobs; average logprob
  #   vs perplexity_threshold decides: stay local (confident) or escalate to cloud (uncertain).
  # perplexity_max_tokens: how many tokens to generate for the probe (default 5). More = more signal, slower.
  # perplexity_threshold: see top-of-section comment; unit is log-probability (negative).
  slm:
    enabled: true
    model: local_models/classifier_0_6b
    mode: perplexity
    # Uncomment when mode: perplexity:
    perplexity_max_tokens: 10
    perplexity_threshold: -0.5   # logprob: avg >= this → local; higher (e.g. -0.3) = more to cloud
# main_llm_language: list of allowed/preferred response languages (e.g. [zh, en] or [en]).
#   - First item = primary: used for prompt file loading (e.g. response.en.yml, prompt_en.yml) and default when user language is unknown.
#   - Full list = allowed response languages: you can add a system-line like "Respond only in one of: {languages}; if the user's language is unknown, use the first."
# How to set: use a YAML list. Examples:
#   main_llm_language: [en]
#   main_llm_language: [zh, en]
#   main_llm_language: [ja, en]
# Single string is still accepted and normalized to a one-element list (e.g. en → [en]).
main_llm_language: [en, zh]

embedding_llm: local_models/embedding_text_model

# For cloud models: set the env var per api_key_name (e.g. OPENAI_API_KEY, ANTHROPIC_API_KEY,
# DEEPSEEK_API_KEY, DASHSCOPE_API_KEY, GROQ_API_KEY, MISTRAL_API_KEY, XAI_API_KEY,
# OPENROUTER_API_KEY, COHERE_API_KEY, PERPLEXITY_API_KEY, GEMINI_API_KEY). Ollama needs none.

# Legacy fields (kept for backward compat; derived from model entry when using local_models/cloud_models)
embedding_host: 127.0.0.1
embedding_port: 5066
# embedding_health_check_timeout_sec: 120   # seconds to wait for local embedding server; default 120; increase for slow machines or large models
main_llm_host: 127.0.0.1
main_llm_port: 5023


# ——— Relational DB (chat history, sessions, runs) ———
# backend: sqlite | mysql | postgresql. For sqlite, url can be empty (uses database/chats.db).
# For MySQL: url: "mysql+pymysql://user:password@host:3306/dbname"
# For PostgreSQL: url: "postgresql+psycopg2://user:password@host:5432/dbname"
database:
  backend: sqlite
  url: ""   # empty = default path (database/chats.db)

# ——— Vector DB (RAG memory) ———
# Used ONLY when memory_backend: chroma. When memory_backend: cognee, Cognee uses its own vector store; configure via Cognee .env (docs.cognee.ai).
# backend: chroma | qdrant | milvus | pinecone | weaviate. Only the selected backend's block is used.
vectorDB:
  backend: chroma
  Chroma:
    anonymized_telemetry: false
    api: chromadb.api.fastapi.FastAPI
    host: 0.0.0.0
    is_persistent: true
    port: 5000
    path: ""   # empty = use data_path (database/)
  # Qdrant (when backend: qdrant)
  Qdrant:
    host: localhost
    port: 6333
    url: ""   # optional: http://localhost:6333
    api_key: ""
  # Milvus (when backend: milvus)
  Milvus:
    host: localhost
    port: 19530
    uri: ""   # optional: http://localhost:19530
  # Pinecone (when backend: pinecone)
  Pinecone:
    api_key: ""
    environment: ""
    index_name: memory
  # Weaviate (when backend: weaviate)
  Weaviate:
    url: http://localhost:8080
    api_key: ""

# ——— Graph DB (entities and relationships for in-house RAG)
# Used ONLY when memory_backend: chroma. When memory_backend: cognee, Cognee uses its own graph store; configure via Cognee .env (docs.cognee.ai).
# backend: kuzu | neo4j. When missing or kuzu not installed, graph is disabled and memory stays vector + relational only.
graphDB:
  backend: kuzu
  Kuzu:
    path: ""   # empty = use data_path/graph_kuzu (e.g. database/graph_kuzu)
  # Neo4j (when backend: neo4j; for enterprise / multi-process)
  Neo4j:
    url: bolt://localhost:7687
    username: neo4j
    password: ""

# ——— Cognee config (used only when memory_backend: cognee)
# We convert these to Cognee env vars at runtime. See docs/MemoryAndDatabase.md.
# SQLite, Chroma, Kuzu: no extra parameters needed (defaults work).
# LLM and embedding: leave empty to use Core's main_llm and embedding_llm (same host/port as chat and embedding).
# By default we set ENABLE_BACKEND_ACCESS_CONTROL=false so Cognee does not require a user (avoids UserNotFoundError).
# To use Cognee's multi-user mode, set cognee.env.ENABLE_BACKEND_ACCESS_CONTROL: "true" and configure Cognee users.
cognee:
  # Relational: sqlite (no params) | postgres (set host, port, username, password)
  relational:
    provider: sqlite
    name: cognee_db
    host: ""
    port: 5432
    username: ""
    password: ""
  # Vector: chroma (no params) | lancedb | qdrant | pgvector | redis | falkordb | neptune_analytics
  vector:
    provider: chroma
    url: ""
    port: ""
    key: ""
  # Graph: kuzu (no params) | kuzu-remote | neo4j | neptune | neptune_analytics
  graph:
    provider: kuzu
    url: ""
    username: ""
    password: ""
  # LLM for Cognee: leave empty to use Core's main_llm (main_llm_host, main_llm_port). Else set provider (e.g. custom), model, endpoint, api_key.
  llm:
    provider: ""
    model: ""
    endpoint: ""
    api_key: ""
  # Embedding for Cognee: leave empty to use Core's embedding_llm (embedding_host, embedding_port). Else set provider, model, endpoint, api_key.
  # tokenizer: local path for token counting (disk only, no HuggingFace at runtime). Resolved on Windows, Mac, Linux. Use ./models/tokenizer or ./models/tokenizer/ModelName (relative to project root) or absolute path.
  embedding:
    provider: ""
    model: ""
    endpoint: ""
    api_key: ""
    tokenizer: "./models/tokenizer/Qwen3_0.6B"
  # Optional: raw Cognee env vars (key: value). Override or add any Cognee env var by name.
  # env:
  #   TELEMETRY_DISABLED: "true"
  #   VECTOR_STORE_PROVIDER: "qdrant"
endpoints: []
# Companion feature: external plugin. Core only routes (enabled, plugin_id, session_id_value, keyword).
# All companion settings (name, character, language, etc.) live in the plugin config; no defaults in Core code.
# keyword is required when enabled: companion routing is disabled until keyword (or name) is set here. Used for message-prefix routing (e.g. "Veda, hi").
# companion:
#   enabled: true
#   plugin_id: friends
#   session_id_value: friend
#   keyword: Veda
