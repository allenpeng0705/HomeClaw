# HomeClaw Core — single config (llama.cpp, local models, cloud models)
# LLM support: (1) Local models via llama.cpp server — OpenAI-compatible HTTP (one server per model, different ports).
#              (2) Cloud models via LiteLLM — same OpenAI-compatible API; set api_key_name and env var per provider.
# Main LLM and embedding can each be local or cloud independently (e.g. local chat + cloud embedding, or both local).
name: core
host: 0.0.0.0
port: 9000
mode: dev
model_path: ../models/
# When false: log [memory], [tools], [skills], [plugin], [TAM], [orchestrator] activity at INFO. When true: suppress those component logs.
silent: false
use_memory: true
reset_memory: false
# memory_backend: cognee (always default) | chroma. Both backends supported. Cognee: configure via cognee: or Cognee .env. Chroma: in-house RAG via core.yml database, vectorDB, graphDB.
# To empty memory for testing: POST or GET http://<core_host>:<core_port>/memory/reset (e.g. curl http://127.0.0.1:9000/memory/reset)
# To empty knowledge base (all users): POST or GET http://<core_host>:<core_port>/knowledge_base/reset
memory_backend: cognee   # default; set to chroma for in-house SQLite+Chroma
# Per-user profile (learned facts: name, birthday, preferences, families, etc.). One JSON file per user under profile.dir (default: database/profiles). See docs/UserProfileDesign.md.
profile:
  enabled: true
  dir: ""   # empty = database/profiles; or set a path for profile JSON files
# Optional: inject workspace bootstrap (IDENTITY.md, AGENTS.md, TOOLS.md) into system prompt; see Comparison.md §7.4
use_workspace_bootstrap: true
# Which workspace dir to load (e.g. config/workspace_day vs config/workspace_night for day/night agents)
workspace_dir: config/workspace
# Optional: inject curated long-term memory from a single Markdown file (AGENT_MEMORY.md). See docs_design/SessionAndDualMemoryDesign.md.
use_agent_memory_file: true
agent_memory_path: ""   # empty = workspace_dir/AGENT_MEMORY.md; or set path relative to project or absolute
agent_memory_max_chars: 5000   # max chars to inject; default 5k. 0 = no truncation; see docs_design/MemoryFilesUsage.md
# When true (recommended): index AGENT_MEMORY.md + daily memory and do NOT inject their content; model uses agent_memory_search + agent_memory_get to pull only relevant parts. Saves context and scales to large files.
use_agent_memory_search: true
agent_memory_vector_collection: homeclaw_agent_memory   # Chroma collection for agent memory chunks
# Optional: inject short-term daily memory (memory/YYYY-MM-DD.md). Load yesterday + today on each request to keep context bounded and avoid filling the context window. Use append_daily_memory tool or write files manually.
use_daily_memory: true
daily_memory_dir: ""    # empty = workspace_dir/memory; or e.g. database/daily_memory
# Session management: pruning, lifecycle, API, dmScope. 
session:
  dm_scope: main              # main | per-peer | per-channel-peer | per-account-channel-peer
  # main (default): all DMs share one session for continuity. per-peer: isolate by sender id across channels.
  # per-channel-peer: isolate by channel + sender (recommended for multi-user inboxes).
  # per-account-channel-peer: isolate by account + channel + sender (multi-account inboxes).
  identity_links: {}          # optional: map canonical id -> list of provider-prefixed peer ids so same person shares DM across channels
  # e.g. { "alice": ["telegram:123", "discord:456"] } — then telegram:123 and discord:456 resolve to "alice" for session key
  prune_keep_last_n: 50       # after each turn (if prune_after_turn) or manually: keep last N turns per session
  prune_after_turn: false     # when true, run transcript prune after each reply (keeps last prune_keep_last_n)
  daily_reset_at_hour: -1     # -1 = disabled; 0-23 = new session when last activity was before today at this hour (local time)
  idle_minutes: -1            # -1 = disabled; >0 = new session when last activity older than N minutes
  api_enabled: true           # expose GET /api/sessions for plugin UIs
# When an unknown user (not in user.yml) tries to access, notify the owner via last-used channel so they can add to user.yml. See docs_design/OutboundMarkdownAndUnknownRequest.md.
notify_unknown_request: false
# Outbound reply: only when the result looks like Markdown, Core converts it before sending; otherwise original text is sent. whatsapp (default) = *bold* _italic_ ~strikethrough~ (most IMs); plain = strip to plaintext (use if channel doesn't support whatsapp format); none = no conversion. See docs_design/OutboundMarkdownAndUnknownRequest.md.
outbound_markdown_format: whatsapp
# Max concurrent LLM calls (channel queue + plugin API /api/plugins/llm/generate).
# Local model: use 1 (default) to avoid overloading one GPU/process.
# Cloud model: may use 2–10 (or per-provider limit) for parallel channel + plugin API; stay under provider RPM/TPM. See docs_design/PluginLLMAndQueueDesign.md.
llm_max_concurrent: 2
# Compaction: trim or summarize context when approaching model limit. Reduces token use and avoids overflows.
compaction:
  enabled: false              # when true, trim/summarize messages before LLM when over limit
  reserve_tokens: 4096        # tokens to leave free for the model's reply (not the same as ctx_size). Should be less than llama_cpp.ctx_size (e.g. 1024–4096); ctx_size is total window, this is just the reserve for output.
  max_messages_before_compact: 30   # max user+assistant+tool messages to keep (older trimmed or summarized)
  compact_tool_results: true  # when true, trim or replace large tool results in the tool loop with a short placeholder when over limit
# Optional: enable tool layer (tool registry; model can call tools by name with args); see Design.md §3.6
use_tools: true
# Optional: inject skills (SKILL.md) from skills_dir into system prompt; see Design.md §3.6
use_skills: true
skills_dir: config/skills
# Cap skills/plugins in prompt: take top N candidates, apply threshold, then up to M in prompt. See docs_design/SessionAndDualMemoryDesign.md.
# Flow: retrieve/load top skills_top_n_candidates (e.g. 10), drop below skills_similarity_threshold, then take up to skills_max_in_prompt (e.g. 5). If only 1, 1 is fine.
skills_top_n_candidates: 10
skills_max_in_prompt: 5
plugins_top_n_candidates: 10
plugins_max_in_prompt: 5
# Max chars per plugin description in the routing block. 0 = no truncation (default). With RAG (plugins_use_vector_search) or plugins_max_in_prompt we already limit how many plugins appear; this only caps per-description length. Use 0 for full descriptions; set 512 or 300 only to shrink prompt or cap one very long description.
plugins_description_max_chars: 0
# Vector retrieval for skills (separate Chroma collection): retrieve by similarity to user query. See docs/ToolsSkillsPlugins.md §8.
skills_use_vector_search: true
skills_vector_collection: homeclaw_skills
skills_max_retrieved: 10
skills_similarity_threshold: 0.5
skills_refresh_on_startup: true
# Optional test folder: full sync every time (id = test__<folder>). Production skills_dir can be incremental (only new).
skills_test_dir: ""
skills_incremental_sync: false
# Optional: when user query matches a regex, ensure these skill folders are in the prompt and optionally append an instruction. No per-skill code in Core; add rules per skill here.
skills_force_include_rules:
  - pattern: "generate\\s+(an?\\s+)?image|create\\s+(an?\\s+)?image|draw\\s+(a\\s+)?(picture|image)"
    folders: [nano-banana-pro-1.0.1]
    instruction: "The user asked to generate or create an image. You MUST call the run_skill tool first (do not reply with text). Use skill_name='nano-banana-pro-1.0.1', script='generate_image.py', args=['--prompt', '<user image description>', '--filename', 'generated.png']. Do not say that no image tool is available."
  - pattern: "summarize\\s+(this\\s+)?(url|link|page|article)"
    folders: [summarize-1.0.0]
    instruction: "The user asked to summarize a URL or page. Use run_skill with skill_name='summarize-1.0.0' and the URL in args, or use the skill's instructions."
# Vector retrieval for plugins (separate collection): same design as skills. RAG finds relevant plugins; only those go into prompt.
plugins_use_vector_search: false
plugins_vector_collection: homeclaw_plugins
plugins_max_retrieved: 10
plugins_similarity_threshold: 0.0
plugins_refresh_on_startup: true
# Optional: when user query matches a regex, ensure these plugin ids are in the routing block and optionally append an instruction. List of { pattern: str, plugins: [str], instruction?: str }. No per-plugin code in Core.
plugins_force_include_rules:
  - pattern: "open\\s+(the\\s+)?browser|navigate\\s+to|go\\s+to\\s+https?://"
    plugins: [homeclaw-browser]
    instruction: "The user asked to open a URL or use the browser. Use route_to_plugin(plugin_id='homeclaw-browser', capability_id='browser_navigate', parameters={\"url\": \"<URL>\"})."
# System plugins (system_plugins/): when true, Core starts each plugin (e.g. node server.js) and runs register (e.g. node register.js) so one command runs Core + all system plugins. See system_plugins/README.md.
system_plugins_auto_start: true   # set true to start homeclaw-browser etc. with Core
system_plugins: [homeclaw-browser]                 # optional allowlist by plugin id (folder name); empty = start all discovered
system_plugins_env:
  homeclaw-browser:
    BROWSER_HEADLESS: false
# Per-plugin env vars (plugin id = folder name under system_plugins/). Each plugin gets only its own env.
# system_plugins_env:
#   homeclaw-browser:
#     BROWSER_HEADLESS: "false"
#   other-plugin:
#     FOO: "bar"
# Orchestrator + TAM + plugins are always enabled. Single choice: routing style.
# orchestrator_unified_with_tools: true (default) — main LLM with tools does routing (route_to_tam / route_to_plugin). One LLM call.
# orchestrator_unified_with_tools: false — separate orchestrator LLM call for intent + plugin selection, then plugin runs.
orchestrator_unified_with_tools: true
# Timeout for orchestrator intent/plugin LLM call and plugin.run() when unified is false. Unit: seconds. 0 = no timeout.
orchestrator_timeout_seconds: 30
# Prompt manager: load prompts from config/prompts with language/model overrides. See docs/PromptManagement.md.
use_prompt_manager: true
prompts_dir: config/prompts
prompt_default_language: en
prompt_cache_ttl_seconds: 0   # Unit: seconds. 0 = cache by file mtime; >0 = TTL
# Optional: require API key for /inbound and /ws when exposing Core on the internet; see docs/RemoteAccess.md
auth_enabled: false
auth_api_key: ""   # when auth_enabled: true, require X-API-Key header or Authorization: Bearer <key>
# Tool layer config (used when use_tools: true). Cross-platform (Mac/Linux/Windows) where possible.
# Built-in tools: time, cron_*, sessions_*, session_status; memory_search, memory_get; file_*,
# folder_list; fetch_url, web_search; browser_*; exec, process_*; image; channel_send; run_skill;
# models_list, agents_list; webhook_trigger; env, cwd, platform_info, echo.
# The keys below are the only ones read by the tool layer; other tools have no configurable options.
tools:
  # exec: allowed command names. Empty or omit = platform default (Unix: ls, cat, pwd; Windows: dir, type, cd). Set explicitly to override.
  exec_allowlist: []   # e.g. [ "date", "whoami", "echo", "pwd", "ls", "cat" ] on Unix; [ "date", "whoami", "echo", "dir", "type", "cd" ] on Windows
  exec_timeout: 30   # Unit: seconds
  # file_read, file_write, etc.: paths UNDER this base only. "." = current working directory (cross-platform). Or use absolute path (e.g. /Users/you/Docs or C:\Users\you\Docs).
  file_read_base: "/Users/shileipeng/Documents/homeclaw"
  # Max characters returned by file_read when the tool does not pass max_chars (default 32000). Increase for long PDFs/documents (e.g. 128000).
  file_read_max_chars: 128000
  run_skill_allowlist: ["run.sh", "main.py", "run.bat"]   # run_skill: if set, only these script names under skill/scripts/. .sh on Windows runs via bash/WSL if available.
  run_skill_timeout: 60   # Unit: seconds
  web:                  # web_search: free (no key) duckduckgo | free tier google_cse (100/day), bing (1000/mo) | tavily, brave, serpapi. Browser: web_search_browser (Google/Bing/Baidu, no key, needs Playwright).
    search:
      provider: tavily  # duckduckgo (no key) | google_cse (100 free/day) | bing (1000 free/mo) | tavily | brave | serpapi. When tavily: Tavily is used first (key under tavily below).
      duckduckgo: {}    # provider=duckduckgo: no API key; pip install duckduckgo-search
      google_cse:       # provider=google_cse: 100 free queries/day
        api_key: ""     # or GOOGLE_CSE_API_KEY
        cx: ""          # Search Engine ID from https://programmablesearchengine.google.com/ (or GOOGLE_CSE_CX)
      bing:             # provider=bing: 1000 free transactions/month (APIs retiring Aug 2025)
        api_key: ""     # or BING_SEARCH_SUBSCRIPTION_KEY (Azure)
      brave:            # when provider=brave
        api_key: ""     # or set BRAVE_API_KEY
        search_type: web   # web (default) | news | video | image
      serpapi:          # when provider=serpapi (paid; ~250 free/mo)
        api_key: ""     # or set SERPAPI_API_KEY
        engine: google # google (default) | bing | baidu
      tavily:          # default provider (free tier: 1000 searches/month). Same key used for search, extract, crawl, research.
        api_key: "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"    # Set here (e.g. tvly-xxx) OR set env TAVILY_API_KEY where Core runs. Get key: https://tavily.com
        search_depth: basic   # search only: basic | fast | advanced | ultra-fast (affects latency vs relevance)
        topic: general        # search only: general | news | finance
        time_range: ""        # search only: day | week | month | year (empty = no filter)
      # When no API key or primary fails (expired/rate limit), use DuckDuckGo (no key). Requires: pip install duckduckgo-search
      fallback_no_key: true   # true = try DuckDuckGo when Tavily/Brave unavailable
      fallback_max_results: 5 # max results from fallback (3–10)
  # --- Browser tools (Core vs external plugin) ---
  # browser_enabled: true  — Core registers built-in browser tools (browser_navigate, browser_snapshot, browser_click, browser_type) when Playwright is installed. The LLM can call these tools directly.
  # browser_enabled: false — Core does NOT register browser tools. Use this when you want browser automation via the system plugin homeclaw-browser (Node.js): register that plugin and set browser_enabled false so the LLM routes all browser actions through route_to_plugin(homeclaw-browser, browser_navigate, ...). Avoids duplicate tools and lets you use the Node.js plugin for canvas/nodes. See system_plugins/homeclaw-browser/README.md.
  browser_enabled: false
  # When browser_enabled true and Playwright available: set to false to show the browser window (local testing only; on servers without a display use true or run under Xvfb).
  browser_headless: false
  # Per-tool execution timeout. Unit: seconds. Prevents a single tool from hanging the system. 0 = no timeout.
  tool_timeout_seconds: 0
# Complex result viewer: save HTML result pages and optionally send a link. One web server for reports (port below); starts/stops with Core. Link = base_url + /result/<id>.html. We do not fetch public IP — you set base_url. Guide: docs/ComplexResultViewerDesign.md § "User guide: making the link work" (Cloudflare tunnel recommended: easy, cheap, stable; or use ngrok / any tunnel).
result_viewer:
  enabled: true
  dir: ""   # empty = database/result_pages
  port: 9001   # report server (http://localhost:9001). Expose via Cloudflare tunnel or similar; that URL = base_url.
  bind_host: "0.0.0.0"
  # URL that reaches the report server (e.g. from cloudflared or ngrok). We append /result/<id>.html. If empty, no link — full response goes to user in chat.
  base_url: "https://homeclaw.gpt4people.online"
  retention_days: 7   # auto-remove files older than this; 0 = keep forever
  max_file_size_kb: 5000
  max_files: 10000

# Optional: user knowledge base (documents, web search, URLs, manual). See docs/MemoryAndDatabase.md.
# backend: auto (default) = use same as memory_backend (cognee -> Cognee DB/vector, chroma -> core.yml vectorDB).
# Set to "cognee" or "chroma" to override (e.g. Cognee memory + built-in RAG for KB).
knowledge_base:
  enabled: true
  backend: auto          # auto | cognee | chroma
  collection_name: homeclaw_kb   # used only when backend is chroma
  chunk_size: 800
  chunk_overlap: 100
  unused_ttl_days: 30   # Unit: days. Cognee: remove sources older than this (age-based); chroma: remove by last_used_timestamp
  max_sources_per_user: 0   # Cognee only: max KB sources per user (0 = no cap). Oldest evicted when over cap.
  embed_timeout: 60   # Unit: seconds
  store_timeout: 60   # Unit: seconds
  # Configurable similarity threshold (0-1, higher = more relevant). Only inject chunks with score >= this; if none pass, no KB block. Set to null to disable (inject top-k regardless).
  retrieval_min_score: null   # e.g. 0.5 or 0.7

# File-understanding (request.files from channels): always inject short notice with paths so the model uses document_read / knowledge_base_add. When user sends file(s) only (no text) and doc is not too big, add to KB directly. See docs_design/FileUnderstandingDesign.md.
file_understanding:
  # When user sends only file(s) (no or negligible text): add extracted document to the user's KB only if extracted text length <= this (chars). If larger, skip; user can say what they want and model uses document_read / knowledge_base_add. 0 = never auto-add (tool-based only).
  add_to_kb_max_chars: 50000   # e.g. 50000; 0 = never add to KB on file-only

# ——— 1. llama.cpp server settings (for local models) ———
# ctx_size, predict, temp are used when starting the llama.cpp server.
# For chat completions, max_tokens and temperature are sent per request (see completion below).
# ctx_size: total context window (input + output). Increase if you see truncation, "context length exceeded",
#   or poor routing when using many plugins/skills (full descriptions), long chat history, or large tool lists.
# Typical values: 32768 (32K, default), 65536 (64K), 131072 (128K). Higher = more VRAM/RAM and may slow inference; check your model's max context.
llama_cpp:
  ctx_size: 32768
  predict: 8192
  temp: 0.7
  threads: 8
  n_gpu_layers: 99
  verbose: false
  repeat_penalty: 1.5
  chat_format: null
  function_calling: true

  embedding:
    ctx_size: 0      # 0 = use model default (recommended; avoids forcing 8192 on models that support less)
    threads: 4
    n_gpu_layers: 20
  
  # Optional: overrides for the embedding server only (local embedding model). Merged over the keys above.
  # Use when the embedding model needs different ctx_size/threads/n_gpu_layers. ctx_size 0 = use model's native n_ctx.
  # embedding:
  #   ctx_size: 0      # 0 = use model default (recommended; avoids forcing 8192 on models that support less)
  #   threads: 8
  #   n_gpu_layers: 99

# ——— Completion (per-request generation) ———
# All parameters below are sent with every chat completion call. Omit or comment out to use server default or llama_cpp fallback where noted.
# Fallbacks: max_tokens → llama_cpp.predict; temperature/top_p/repeat_penalty → llama_cpp.temp / (none) / llama_cpp.repeat_penalty.
completion:
  max_tokens: 8192          # max tokens to generate per response (fallback: llama_cpp.predict)
  temperature: 0.7          # sampling temperature 0..2 (fallback: llama_cpp.temp)
  top_p: 1.0                # nucleus sampling 0..1
  presence_penalty: 0       # OpenAI-style; penalize tokens that appear in text so far
  frequency_penalty: 0      # OpenAI-style; penalize by token frequency
  repeat_penalty: 1.5       # llama.cpp-style; sent in extra_body (fallback: llama_cpp.repeat_penalty)
  seed: null                 # optional; int for reproducible sampling
  stop: null                 # optional; list of strings, e.g. ["</s>", "Human:"]
  # logit_bias: {}          # optional; dict of token id to bias (-100..100)
  # n: 1                    # number of completions (default 1)
  # response_format: null   # optional; e.g. { "type": "json_object" }
  # timeout: null            # optional; Unit: seconds
  image_max_dimension: 512   # 0 = no resize. Set e.g. 1024 to resize images before sending to vision model (keeps aspect ratio; requires Pillow)

# ——— 2. Local models (llama.cpp server per model; path relative to model_path) ———
# Optional per model: mmproj (vision), lora (adapter(s)), lora_base (optional base for LoRA).
#   mmproj: path to projector .gguf; llama-server -m <path> --mmproj <mmproj>
#   lora: single path or array of paths to LoRA adapter .gguf; each gets --lora <path> (multi-LoRA if supported)
#   lora_base: optional; only when the adapter requires a specific base; --lora-base <path>
# Optional supported_media: list of media types the model can handle — [image], [image, audio], or [image, audio, video].
#   If omitted: local model defaults to [] (text-only) unless mmproj is set, then [image]. Cloud model defaults to [image, audio, video].
#   Use supported_media to override (e.g. restrict cloud to [image] or enable image for a local model without mmproj).
# Paths are relative to model_path (same as path).
local_models:
  - id: embedding_text_model
    alias: embedding
    path: Qwen3-Embedding-0.6B-Q8_0.gguf  #Qwen3-Embedding-4B-Q4_K_M.gguf,  bge-m3-Q5_K_M.gguf
    host: 127.0.0.1
    port: 5066
    capabilities: [embedding]
  # Example vision model (uncomment and set paths): mmproj required; supported_media defaults to [image]
  # - id: llava_1_5_7b
  #   alias: LLaVA 1.5 7B
  #   path: llava/llava-v1.5-7b-model.gguf
  #   mmproj: llava/llava-v1.5-7b-mmproj.gguf
  #   # supported_media: [image]   # optional; default [image] when mmproj is set
  #   host: 127.0.0.1
  #   port: 5020
  #   capabilities: [Chat]
  # Example LoRA (uncomment): lora (single path or array), lora_base optional
  # - id: base_with_lora
  #   alias: Base + LoRA
  #   path: base/base-model.gguf
  #   lora: adapters/adapter-01.gguf
  #   # lora_base: base/base-model.gguf   # optional; only if adapter needs a specific base
  #   host: 127.0.0.1
  #   port: 5021
  #   capabilities: [Chat]
  # Multi-LoRA (if llama-server supports multiple --lora): use array
  #   lora: [adapters/a1.gguf, adapters/a2.gguf]
  - id: main_vl_model_4B
    alias: main_vl_model_4B
    path: Qwen3VL-4B-Instruct-Q4_K_M.gguf 
    mmproj: mmproj-Qwen3VL-4B-Instruct-F16.gguf
    host: 127.0.0.1
    port: 5023
    capabilities: [Chat]
    supported_media: [image]

  - id: main_vl_model_8B
    alias: main_vl_model_8B
    path: Qwen3-VL-8B-Instruct-Q4_K_M.gguf
    mmproj: mmproj-Qwen3-VL-8B-BF16.gguf
    host: 127.0.0.1
    port: 5024
    capabilities: [Chat]
    supported_media: [image]

  - id: gemma_3_4b_it_4B
    alias: gemma_3_4b_it_4B
    path: gemma-3-4b-it-Q4_K_M.gguf
    mmproj: mmproj-gemma-3-4b-BF16.gguf
    host: 127.0.0.1
    port: 5025
    capabilities: [Chat]
    supported_media: [image]

  # Optional: Layer 3 classifier for mix mode (hybrid_router.slm.model). Small model (e.g. Qwen3-0.5B) on a separate port; Core starts it when main_llm_mode: mix and slm.enabled.
  - id: classifier_0_6b
    alias: classifier
    path: Qwen3-0.6B-Q5_K_M.gguf
    host: 127.0.0.1
    port: 5089
    capabilities: [Chat]


# ——— 3. Cloud models (LiteLLM; one service per entry, each host/port) ———
# api_key_name: env var name (e.g. GEMINI_API_KEY); Core uses that env var for the key. Optional api_key: set the key here instead of env (use for convenience only; avoid committing secrets).
# Start LiteLLM per model or use one proxy. See https://github.com/BerriAI/litellm and docs.litellm.ai/docs/providers
# Use as main LLM (Chat/Functioncall) or as embedding model (capabilities: [embedding]); main and embedding can mix local + cloud.
# Optional supported_media per entry: [image], [image, audio], or [image, audio, video]. If omitted, cloud models default to [image, audio, video].
cloud_models:
  # OpenAI (chat and embedding). supported_media: gpt-4o supports image only; gpt-4o-audio-preview supports audio only (see MediaSupportAndProviders.md).
  - id: OpenAI-GPT4o
    alias: OpenAI GPT-4o
    path: openai/gpt-4o
    host: 127.0.0.1
    port: 4001
    api_key_name: OPENAI_API_KEY
    supported_media: [image]   # gpt-4o = vision only; audio is a different model (gpt-4o-audio-preview)
    capabilities: [Functioncall, Chat]
  - id: OpenAI-GPT4o-mini
    alias: OpenAI GPT-4o mini
    path: openai/gpt-4o-mini
    host: 127.0.0.1
    port: 4001
    api_key_name: OPENAI_API_KEY
    # supported_media: [image, audio, video]   # optional; cloud default is all
    capabilities: [Functioncall, Chat]
  # Google Gemini
  - id: Gemini-2.5-Flash
    alias: Gemini 2.5 Flash
    path: gemini/gemini-2.5-flash
    host: 127.0.0.1
    port: 4002
    api_key_name: GEMINI_API_KEY
    api_key: 
    capabilities: [Functioncall, Chat]
  # Anthropic Claude
  - id: Claude-Sonnet
    alias: Claude 3.5 Sonnet
    path: anthropic/claude-3-5-sonnet-20241022
    host: 127.0.0.1
    port: 4003
    api_key_name: ANTHROPIC_API_KEY
    capabilities: [Functioncall, Chat]
  # Ollama (local; no API key)
  - id: Ollama-qwen2
    alias: Ollama Qwen2
    path: ollama/qwen2.5:7b
    host: 127.0.0.1
    port: 4004
    capabilities: [Chat]
  # DeepSeek
  - id: DeepSeek-Coder
    alias: DeepSeek Coder
    path: deepseek/deepseek-coder
    host: 127.0.0.1
    port: 4005
    api_key_name: DEEPSEEK_API_KEY
    capabilities: [Chat]
  # Alibaba Qwen (Dashscope)
  - id: Qwen-Turbo
    alias: Qwen Turbo (Dashscope)
    path: dashscope/qwen-turbo
    host: 127.0.0.1
    port: 4006
    api_key_name: DASHSCOPE_API_KEY
    capabilities: [Chat]
  # Groq
  - id: Groq-Llama70B
    alias: Groq Llama 3.1 70B
    path: groq/llama-3.1-70b-versatile
    host: 127.0.0.1
    port: 4007
    api_key_name: GROQ_API_KEY
    capabilities: [Chat]
  # Mistral
  - id: Mistral-Large
    alias: Mistral Large
    path: mistral/mistral-large-latest
    host: 127.0.0.1
    port: 4008
    api_key_name: MISTRAL_API_KEY
    capabilities: [Functioncall, Chat]
  # xAI (Grok)
  - id: xAI-Grok
    alias: xAI Grok 2
    path: xai/grok-2
    host: 127.0.0.1
    port: 4009
    api_key_name: XAI_API_KEY
    capabilities: [Chat]
  # OpenRouter (many models via one API)
  - id: OpenRouter-GPT4o
    alias: OpenRouter GPT-4o
    path: openrouter/openai/gpt-4o
    host: 127.0.0.1
    port: 4010
    api_key_name: OPENROUTER_API_KEY
    capabilities: [Functioncall, Chat]
  # Cohere
  - id: Cohere-CommandR
    alias: Cohere Command R+
    path: cohere/command-r-plus
    host: 127.0.0.1
    port: 4011
    api_key_name: COHERE_API_KEY
    capabilities: [Chat]
  # Perplexity (search-augmented)
  - id: Perplexity-Sonar
    alias: Perplexity Sonar
    path: perplexity/llama-3.1-sonar-small-128k-online
    host: 127.0.0.1
    port: 4012
    api_key_name: PERPLEXITY_API_KEY
    capabilities: [Chat]

# ——— Media support (image / audio / video) for main_llm ———
# See docs_design/MediaSupportAndProviders.md for how LiteLLM, OpenAI, and Gemini support image/audio/video.
# WHERE TO SET: Add optional "supported_media" on the model entry that you use as main_llm (that entry is under
#   local_models: or cloud_models: below; main_llm points to it, e.g. main_llm: cloud_models/OpenAI-GPT4o).
# CLOUD: We do not call the API to discover capabilities. We default cloud to [image, audio, video]; for OpenAI
#   you must set supported_media per model: gpt-4o supports image only → supported_media: [image]; gpt-4o-audio-preview
#   supports audio only → supported_media: [audio]. Gemini (e.g. 1.5 Pro) often supports all → [image, audio, video].
# LOCAL: Default [] (text-only); if the model has mmproj (vision), default [image]. Override with supported_media if needed.
# HOW IT WORKS: Core calls main_llm_supported_media() and only sends image/audio/video parts in that list; others
#   are omitted and a short note is added so the model does not crash.

# ——— Selected main and embedding model: local_models/<id> or cloud_models/<id> ———
# main_llm: optional when main_llm_mode is set. When mode is "local" we use main_llm_local; when "cloud" we use main_llm_cloud; when "mix" we use both per request. Set main_llm only if you omit main_llm_mode (legacy: derive mode from main_llm).
#main_llm: local_models/main_vl_model_4B
main_llm_mode: mix
main_llm_local: local_models/main_vl_model_4B
main_llm_cloud: cloud_models/Gemini-2.5-Flash
hybrid_router:
  default_route: local
  show_route_in_response: true
  # When true (default): if the chosen model fails (timeout/error), retry once with the other route so one model failing does not block the task.
  fallback_on_llm_error: true
  heuristic:
    enabled: true
    threshold: 0.5
    rules_path: config/hybrid/heuristic_rules.yml
  semantic:
    enabled: true
    threshold: 0.6
    routes_path: config/hybrid/semantic_routes.yml
  # Layer 3: classifier (small model) or perplexity (main model confidence probe).
  # - mode: classifier = small model answers "Local or Cloud?" (uses model above).
  # - mode: perplexity = main local model generates a few tokens with logprobs; average logprob
  #   vs perplexity_threshold decides: stay local (confident) or escalate to cloud (uncertain).
  # perplexity_max_tokens: how many tokens to generate for the probe (default 5). More = more signal, slower.
  # perplexity_threshold: avg logprob above this → local, below → cloud. Higher (e.g. -0.4) = stricter local; lower (e.g. -0.8) = more stays local. Default -0.6.
  slm:
    enabled: true
    threshold: 0.5
    model: local_models/classifier_0_6b
    mode: classifier
    # Uncomment when mode: perplexity:
    # perplexity_max_tokens: 5
    # perplexity_threshold: -0.6
# main_llm_language: list of allowed/preferred response languages (e.g. [zh, en] or [en]).
#   - First item = primary: used for prompt file loading (e.g. response.en.yml, prompt_en.yml) and default when user language is unknown.
#   - Full list = allowed response languages: you can add a system-line like "Respond only in one of: {languages}; if the user's language is unknown, use the first."
# How to set: use a YAML list. Examples:
#   main_llm_language: [en]
#   main_llm_language: [zh, en]
#   main_llm_language: [ja, en]
# Single string is still accepted and normalized to a one-element list (e.g. en → [en]).
main_llm_language: [en, zh]

embedding_llm: local_models/embedding_text_model

# For cloud models: set the env var per api_key_name (e.g. OPENAI_API_KEY, ANTHROPIC_API_KEY,
# DEEPSEEK_API_KEY, DASHSCOPE_API_KEY, GROQ_API_KEY, MISTRAL_API_KEY, XAI_API_KEY,
# OPENROUTER_API_KEY, COHERE_API_KEY, PERPLEXITY_API_KEY, GEMINI_API_KEY). Ollama needs none.

# Legacy fields (kept for backward compat; derived from model entry when using local_models/cloud_models)
embedding_host: 127.0.0.1
embedding_port: 5066
main_llm_host: 127.0.0.1
main_llm_port: 5023


# ——— Relational DB (chat history, sessions, runs) ———
# backend: sqlite | mysql | postgresql. For sqlite, url can be empty (uses database/chats.db).
# For MySQL: url: "mysql+pymysql://user:password@host:3306/dbname"
# For PostgreSQL: url: "postgresql+psycopg2://user:password@host:5432/dbname"
database:
  backend: sqlite
  url: ""   # empty = default path (database/chats.db)

# ——— Vector DB (RAG memory) ———
# Used ONLY when memory_backend: chroma. When memory_backend: cognee, Cognee uses its own vector store; configure via Cognee .env (docs.cognee.ai).
# backend: chroma | qdrant | milvus | pinecone | weaviate. Only the selected backend's block is used.
vectorDB:
  backend: chroma
  Chroma:
    anonymized_telemetry: false
    api: chromadb.api.fastapi.FastAPI
    host: 0.0.0.0
    is_persistent: true
    port: 5000
    path: ""   # empty = use data_path (database/)
  # Qdrant (when backend: qdrant)
  Qdrant:
    host: localhost
    port: 6333
    url: ""   # optional: http://localhost:6333
    api_key: ""
  # Milvus (when backend: milvus)
  Milvus:
    host: localhost
    port: 19530
    uri: ""   # optional: http://localhost:19530
  # Pinecone (when backend: pinecone)
  Pinecone:
    api_key: ""
    environment: ""
    index_name: memory
  # Weaviate (when backend: weaviate)
  Weaviate:
    url: http://localhost:8080
    api_key: ""

# ——— Graph DB (entities and relationships for in-house RAG)
# Used ONLY when memory_backend: chroma. When memory_backend: cognee, Cognee uses its own graph store; configure via Cognee .env (docs.cognee.ai).
# backend: kuzu | neo4j. When missing or kuzu not installed, graph is disabled and memory stays vector + relational only.
graphDB:
  backend: kuzu
  Kuzu:
    path: ""   # empty = use data_path/graph_kuzu (e.g. database/graph_kuzu)
  # Neo4j (when backend: neo4j; for enterprise / multi-process)
  Neo4j:
    url: bolt://localhost:7687
    username: neo4j
    password: ""

# ——— Cognee config (used only when memory_backend: cognee)
# We convert these to Cognee env vars at runtime. See docs/MemoryAndDatabase.md.
# SQLite, Chroma, Kuzu: no extra parameters needed (defaults work).
# LLM and embedding: leave empty to use Core's main_llm and embedding_llm (same host/port as chat and embedding).
# By default we set ENABLE_BACKEND_ACCESS_CONTROL=false so Cognee does not require a user (avoids UserNotFoundError).
# To use Cognee's multi-user mode, set cognee.env.ENABLE_BACKEND_ACCESS_CONTROL: "true" and configure Cognee users.
cognee:
  # Relational: sqlite (no params) | postgres (set host, port, username, password)
  relational:
    provider: sqlite
    name: cognee_db
    host: ""
    port: 5432
    username: ""
    password: ""
  # Vector: chroma (no params) | lancedb | qdrant | pgvector | redis | falkordb | neptune_analytics
  vector:
    provider: chroma
    url: ""
    port: ""
    key: ""
  # Graph: kuzu (no params) | kuzu-remote | neo4j | neptune | neptune_analytics
  graph:
    provider: kuzu
    url: ""
    username: ""
    password: ""
  # LLM for Cognee: leave empty to use Core's main_llm (main_llm_host, main_llm_port). Else set provider (e.g. custom), model, endpoint, api_key.
  llm:
    provider: ""
    model: ""
    endpoint: ""
    api_key: ""
  # Embedding for Cognee: leave empty to use Core's embedding_llm (embedding_host, embedding_port). Else set provider, model, endpoint, api_key.
  # tokenizer: local path or HuggingFace model id for token counting (silences "Could not automatically map embedding_text_model to a tokeniser").
  embedding:
    provider: ""
    model: ""
    endpoint: ""
    api_key: ""
    tokenizer: "../models/tokenizer/Qwen3_0.6B"
  # Optional: raw Cognee env vars (key: value). Override or add any Cognee env var by name.
  # env:
  #   TELEMETRY_DISABLED: "true"
  #   VECTOR_STORE_PROVIDER: "qdrant"
endpoints: []
