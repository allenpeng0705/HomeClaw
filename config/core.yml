# HomeClaw Core — single config (llama.cpp, local models, cloud models)
# LLM support: (1) Local models via llama.cpp server — OpenAI-compatible HTTP (one server per model, different ports).
#              (2) Cloud models via LiteLLM — same OpenAI-compatible API; set api_key_name and env var per provider.
# Main LLM and embedding can each be local or cloud independently (e.g. local chat + cloud embedding, or both local).
name: core
host: 0.0.0.0
port: 9000
mode: dev
# Portal (Phase 3): when set, Core proxies config and portal APIs to Portal; Companion can load Portal UI via Core. Generate portal_secret once (e.g. 32 chars), set same in Portal: PORTAL_SECRET env or config/portal_secret.txt.
# portal_url: "http://127.0.0.1:18472"
# portal_secret: ""
# model_path: base path for local models (GGUF, etc.). Relative to project root; resolved on Windows, Mac, and Linux (use / in config; pathlib normalizes). Default: models (project root). Use ../models to put models outside the repo.
model_path: models
# When false: log [memory], [tools], [skills], [plugin], [TAM], [orchestrator] activity at INFO (verbose). When true: only INFO to console (less noise). On Windows CMD, use true to avoid huge output and "Press Space to continue" blocking.
silent: false
# When false: all logs go only to the log file (logs/core_debug.log); nothing to console. To monitor: "Get-Content logs\core_debug.log -Wait -Tail 50 -Encoding UTF8" (PowerShell; -Encoding UTF8 needed for Chinese/special chars on Windows) or "tail -f logs/core_debug.log" (Unix). When true: logs also go to console.
log_to_console: false
# Memory, knowledge base, database: moved to memory_kb.yml. Set path below; edit config/memory_kb.yml.
memory_kb_config_file: memory_kb.yml
# default_location: optional fallback when no request/profile location (e.g. "New York, US"); see docs_design/SystemContextDateTimeAndLocation.md
# Optional: inject workspace bootstrap (IDENTITY.md, AGENTS.md, TOOLS.md) into system prompt. Default when missing: true.
use_workspace_bootstrap: true
# Which workspace dir to load (e.g. config/workspace_day vs config/workspace_night for day/night agents)
workspace_dir: config/workspace
# Root for file/folder tools and links. Required for channel/companion file access. When empty, file tools return a clear error; do not use workspace for user files. Each user has two major folders: (1) their sandbox homeclaw_root/{user_id}/ (private), (2) the share folder homeclaw_root/share/ (accessible by all users). Companion app uses homeclaw_root/companion/. Subfolders: output/, knowledgebase/.
homeclaw_root: "/Users/shileipeng/Documents/homeclawroot"
# Session management: in config/memory_kb.yml (see memory_kb_config_file above).
# When an unknown user (not in user.yml) tries to access, notify the owner via last-used channel so they can add to user.yml. See docs_design/OutboundMarkdownAndUnknownRequest.md.
notify_unknown_request: false
# Outbound reply: only when the result looks like Markdown, Core converts it before sending; otherwise original text is sent. whatsapp (default) = *bold* _italic_ ~strikethrough~ (most IMs); plain = strip to plaintext (use if channel doesn't support whatsapp format); none = no conversion. See docs_design/OutboundMarkdownAndUnknownRequest.md.
outbound_markdown_format: whatsapp
# Max concurrent LLM calls (channel queue + plugin API /api/plugins/llm/generate). Separate limits for local (llama.cpp) and cloud (LiteLLM). See docs_design/PluginLLMAndQueueDesign.md.
llm_max_concurrent_local: 1   # local (llama.cpp): 1 = typical for single GPU/process
llm_max_concurrent_cloud: 4   # cloud (LiteLLM): 2–10 for parallel channel + plugin; stay under provider RPM/TPM
# Compaction: trim or summarize context when approaching model limit. Reduces token use and avoids overflows.
compaction:
  enabled: false              # when true, trim/summarize messages before LLM when over limit
  reserve_tokens: 4096        # tokens to leave free for the model's reply (not the same as ctx_size). Should be less than llama_cpp.ctx_size (e.g. 1024–4096); ctx_size is total window, this is just the reserve for output.
  max_messages_before_compact: 30   # max user+assistant+tool messages to keep (older trimmed or summarized)
  compact_tool_results: true  # when true, trim or replace large tool results in the tool loop with a short placeholder when over limit
  # Single flag: when true, memory is written only in a dedicated flush turn before compaction; main prompt does not ask the model to call append_*. Omit or set true for default; set false to use the old behavior (model writes during the turn).
  memory_flush_primary: true
  memory_flush_prompt: "Pre-compaction memory flush. From the conversation so far, store durable memories now. Use append_agent_memory for lasting facts and append_daily_memory for today's short-term notes. APPEND only; do not overwrite. If nothing to store, reply briefly with 'Nothing to store.'"
# Tools and skills are always enabled. skills_dir and tool config: config/skills_and_plugins.yml (and tools: there).
skills_dir: skills
# Optional extra dirs for skills (paths relative to project root). User can put more skill folders here; merged with skills_dir (first wins by folder name). See docs_design/SkillsAndPluginsSamePolicy.md.
# skills_extra_dirs: [external_skills]
# Folder names to not load (case-insensitive). Use to disable a skill without removing it (e.g. ["x-api-1.0.0"]).
# skills_disabled: []
# Skills and plugins: all skills_*, plugins_*, system_plugins* settings live in a separate file so core.yml stays short.
# Path is relative to the directory containing core.yml (config/). Edit config/skills_and_plugins.yml.
skills_and_plugins_config_file: skills_and_plugins.yml
# Orchestrator + TAM + plugins are always enabled. Single choice: routing style.
# orchestrator_unified_with_tools: true (default) — main LLM with tools does routing (route_to_tam / route_to_plugin). One LLM call.
# orchestrator_unified_with_tools: false — separate orchestrator LLM call for intent + plugin selection, then plugin runs.
# orchestrator_unified_with_tools: true
# Timeout for orchestrator intent/plugin LLM call and plugin.run() when unified is false. Unit: seconds. 0 = no timeout.
# orchestrator_timeout_seconds: 60
# Recommended max time (seconds) for clients/proxies waiting for Core to respond (e.g. /inbound). 0 = unlimited (no recommended limit). Long tasks (document_read, HTML slides, summarization) may take 2+ minutes; "Connection closed while receiving data" usually means proxy/client read_timeout was exceeded. Fix: use POST /inbound with stream: true (SSE) and/or set proxy read_timeout >= 300. Proxies (nginx, Cloudflare) should set read_timeout >= this when >0.
# inbound_request_timeout_seconds: 0
# Prompt manager: load prompts from config/prompts with language/model overrides. See docs/PromptManagement.md.
use_prompt_manager: true
prompts_dir: config/prompts
prompt_default_language: en
prompt_cache_ttl_seconds: 0   # Unit: seconds. 0 = cache by file mtime; >0 = TTL
# Optional: require API key for /inbound and /ws when exposing Core on the internet; see docs/RemoteAccess.md
auth_enabled: true
auth_api_key: "1234567890"   # when auth_enabled: true, require X-API-Key header or Authorization: Bearer <key>. Also used to sign file/report links (GET /files/out) when core_public_url is set.
# Public URL that reaches this Core instance (e.g. https://homeclaw.example.com via Cloudflare Tunnel). Used for: (1) file/report links (2) GET /pinggy scan-to-connect page and QR for Companion. Leave empty for local-only or if using pinggy.token for tunnel.
core_public_url: "https://homeclaw.gpt4people.online"
# File link style: "token" (default) = /files/out?token=... ; "static" = path in URL, link = core_public_url/file_static_prefix/scope/path?token=... (token still required so link only accesses that user's sandbox). See docs/FileLinkStatic.md.
file_link_style: token
file_static_prefix: files
# How long file/view links (token) are valid. Seconds (e.g. 604800) or days (e.g. "7d"). Default 7 days; max 365 days.
file_view_link_expiry_sec: 604800
# Pinggy tunnel: when token is set, Core starts a tunnel to this port and serves /pinggy with public URL + QR for Companion. open_browser: true = open browser to /pinggy when tunnel is ready. See docs_design/PinggyIntegration.md.
pinggy:
  token: ""        # empty = do not start Pinggy; set to your Pinggy token to enable
  open_browser: true   # when true, open default browser to http://127.0.0.1:<port>/pinggy when tunnel is ready

# Push notifications: APNs for iOS/macOS, FCM for Android (Companion reminders when app is backgrounded/killed).
# APNs: pip install pyjwt cryptography "httpx[http2]". Use .p8 key from Apple Developer → Keys (not .p12 cert).
# FCM: pip install firebase-admin; use a Firebase service account JSON key.
push_notifications:
  enabled: true
  ios:  # also used for macOS (same bundle_id for Companion iOS/macOS app)
    key_path: "config/AuthKey_4S2J822AM4.p8"   # path to .p8 file (relative to project root)
    key_id: "4S2J822AM4"      # Key ID from Apple Developer (e.g. ABC123XYZ)
    team_id: "VW379TB6DM"     # Team ID (10 chars, from Apple Developer membership)
    bundle_id: "com.homeclaw.homeclawApp"   # Companion app bundle ID (iOS + macOS)
    sandbox: true   # true for development/TestFlight, false for App Store
  # FCM (Android and other non-Apple). Optional: set GOOGLE_APPLICATION_CREDENTIALS env instead.
  fcm:
    credentials_path: ""   # path to serviceAccountKey.json (relative to project root), or leave empty to use env

# Tool layer (use_tools: true): tools section is in skills_and_plugins.yml. Edit config/skills_and_plugins.yml under "tools:".
# Knowledge base and memory/DB blocks are in memory_kb.yml (see memory_kb_config_file above).
# File-understanding (request.files from channels): always inject short notice with paths so the model uses document_read / knowledge_base_add. When user sends file(s) only (no text) and doc is not too big, add to KB directly. See docs_design/FileUnderstandingDesign.md.
file_understanding:
  # When user sends only file(s) (no or negligible text): add extracted document to the user's KB only if extracted text length <= this (chars). If larger, skip; user can say what they want and model uses document_read / knowledge_base_add. 0 = never auto-add (tool-based only).
  add_to_kb_max_chars: 5000000   # e.g. 50000; 0 = never add to KB on file-only

# ——— 1. llama.cpp server settings (for local models) ———
# ctx_size, predict, temp are used when starting the llama.cpp server.
# For chat completions, max_tokens and temperature are sent per request (see completion below).
# ctx_size: total context window (input + output). Increase if you see truncation, "context length exceeded",
#   or poor routing when using many plugins/skills (full descriptions), long chat history, or large tool lists.
# Typical values: 32768 (32K, default), 65536 (64K), 131072 (128K). Higher = more VRAM/RAM and may slow inference; check your model's max context.
llama_cpp:
  ctx_size: 32768
  predict: 8192
  temp: 0.7
  threads: 8
  n_gpu_layers: 99
  verbose: false
  repeat_penalty: 1.5
  chat_format: null
  function_calling: true

  embedding:
    ctx_size: 0      # 0 = use model default (recommended; avoids forcing 8192 on models that support less)
    threads: 4
    n_gpu_layers: 20
  
  # Optional: overrides for the embedding server only (local embedding model). Merged over the keys above.
  # Use when the embedding model needs different ctx_size/threads/n_gpu_layers. ctx_size 0 = use model's native n_ctx.
  # embedding:
  #   ctx_size: 0      # 0 = use model default (recommended; avoids forcing 8192 on models that support less)
  #   threads: 8
  #   n_gpu_layers: 99

# ——— Completion (per-request generation) ———
# All parameters below are sent with every chat completion call. Omit or comment out to use server default or llama_cpp fallback where noted.
# Fallbacks: max_tokens → llama_cpp.predict; temperature/top_p/repeat_penalty → llama_cpp.temp / (none) / llama_cpp.repeat_penalty.
completion:
  max_tokens: 8192          # max tokens to generate per response (fallback: llama_cpp.predict)
  temperature: 0.7          # sampling temperature 0..2 (fallback: llama_cpp.temp)
  top_p: 1.0                # nucleus sampling 0..1
  presence_penalty: 0       # OpenAI-style; penalize tokens that appear in text so far
  frequency_penalty: 0      # OpenAI-style; penalize by token frequency
  repeat_penalty: 1.5       # llama.cpp-style; sent in extra_body (fallback: llama_cpp.repeat_penalty)
  seed: null                 # optional; int for reproducible sampling
  stop: null                 # optional; list of strings, e.g. ["</s>", "Human:"]
  # logit_bias: {}          # optional; dict of token id to bias (-100..100)
  # n: 1                    # number of completions (default 1)
  # response_format: null   # optional; e.g. { "type": "json_object" }
  # timeout: null            # optional; Unit: seconds
  image_max_dimension: 512   # 0 = no resize. Set e.g. 1024 to resize images before sending to vision model (keeps aspect ratio; requires Pillow)

# local_models, cloud_models, main_llm*, hybrid_router, embedding_llm: in config/llm.yml (see llm_config_file below).
llm_config_file: llm.yml

# database, vectorDB, graphDB, cognee: in config/memory_kb.yml (see memory_kb_config_file above).
endpoints: []
# Companion feature: external plugin. Core only routes (enabled, plugin_id, session_id_value, keyword).
# All companion settings (name, character, language, etc.) live in the plugin config; no defaults in Core code.
# keyword is required when enabled: companion routing is disabled until keyword (or name) is set here. Used for message-prefix routing (e.g. "Veda, hi").
# companion:
#   enabled: true
#   plugin_id: friends
#   session_id_value: friend
#   keyword: Veda
