# 我为什么以及如何做了 HomeClaw：一个故事

*从 GPT4People 到 HomeClaw——本地 LLM、渠道、记忆、插件，以及向 OpenClaw 学习。*

---

我想讲讲 **HomeClaw** 的故事：为什么开始、如何演变、以及接下来会怎样。它始于半年多前的 **GPT4People**。如今它是一个自托管的 AI 助手，在家运行，通过你已有的渠道与你对话，并把数据留在你这边。下面是这段经历。

---

## 为什么开始：安全与简单

一开始我想要两件事。

**第一：用本地 LLM 保证安全。** 我不想每次对话都发到别人的云端。我希望模型在自己机器上跑，这样我说的话就留在我这里。所以核心想法是：**本地 LLM 优先**。

**第二：用简单的方式从任何地方触达这台 LLM。** 在家跑的模型，如果必须坐在那台电脑前才能用，就没意义。我想要**渠道**——从手机、从公司、从任何地方和家里的电脑对话，并通过这些渠道简单使用本地 LLM，不用复杂配置。所以：**通过渠道触达家中电脑，简单使用本地 LLM**。

这两点——本地 LLM + 渠道——成了项目的核心。我把它叫做 **GPT4People**。

---

## 起步：聊天与简单记忆

起初我专注于**聊天**。从渠道收消息，发给本地 LLM，拿到回复，再发回去。为了让对话在多次会话中有用，我加了一个**简单的基于 RAG 的记忆**：把过去的轮次嵌入、存储，在回答时检索相关片段。这样助手就能说“上次你提到……”而不用我每次都重讲一遍。

我还接上了**主流 IM**，让大家用平时和好友聊天的方式和助手说话：**微信**、**WhatsApp** 等。想法是：你给“家庭助手”发消息，就像给任何人发消息一样。不需要新应用、新习惯——只是多了一个联系人，恰好是你本地的 AI。

所以早期我们有：本地 LLM、渠道（微信、WhatsApp 等）和一个小型 RAG 记忆。这对日常聊天和轻度回忆已经够用。

---

## 现实检验：家用电脑并不总是够用

一段时间后遇到一个实际问题：**家用电脑算力往往有限**。没有 GPU，或只有老显卡。在本地跑大模型很慢甚至跑不动。有的用户只有笔记本，有的用 NAS 或小服务器。强推“只用本地”会限制谁能用。

于是我想：要是**也支持云端模型**呢？能本地就本地，机器太弱或任务太重时再用云端 API（OpenAI、Gemini、DeepSeek 等）。我通过 **LiteLLM** 加了**云端模型支持**，这样同一个助手可以指向本地模型或云端模型——或两者兼用、分工不同（例如聊天用本地、嵌入用云端）。既保留“能本地就本地”的思路，又让更多硬件能真正用起来。

---

## 做实事：插件（以及扩展的成本）

聊天和记忆不错，但我希望助手**能做实事**：“帮我订酒店”“明天 9 点提醒我”“天气怎么样？”于是我设计了一个**简单的插件框架**：一个插件 = 一种能力。插件可以**热加载**、**扩展**，不用重启整个系统。LLM 判断用户请求是否匹配某个插件并路由过去；插件运行（调 API、发邮件等）并返回结果。

框架好用。但**每种新能力都要写一个新插件**。天气、邮件、订房——每一个都要设计开发。扩展性是有的，但增加功能的成本仍在开发者身上。我记着这一点，后面需要既有清晰的插件模型，也有办法复用别人已经做好的东西。

---

## 从一开始：多用户与多模型

有两个设计选择从一开始就定下并一直保留。

**多用户与权限控制。** 我希望多个人（比如一家人）共用同一个 HomeClaw 实例，但彼此看不到对方的数据。所以从第一天就做了**多用户**和**权限控制**，由一个**简单配置文件**驱动（现在是 `config/user.yml`）。每个用户有一个身份（邮箱、IM id 等）；对话历史和记忆按用户隔离。没有复杂的认证服务——就是清晰的、基于文件的允许列表和存储上的严格隔离。

**多模型。** 我也把系统设计成可以**按需加载和使用多个模型**：一个聊天、一个嵌入，以后还可以按任务用不同模型。这样可以在不重写核心的情况下混用本地与云端、小模型与大模型。正是这个设计让“后面再加云端”和“一个智能体、多种 LLM”成为可能。

---

## OpenClaw 火了：学习与保留已有成果

最近 **OpenClaw** 受到很多关注。我看了之后发现**很多想法和我一直在做的很像**：通过渠道触达助手、家用电脑作为 AI 运行的地方、LLM 替你做事（工具、技能）。这很鼓舞——说明方向在别人那里也说得通。

我决定**向 OpenClaw 学习**，但不丢掉 GPT4People 里已经好用的部分。OpenClaw 有丰富的**技能**生态（SKILL.md、工作流、ClawHub）。于是我让 HomeClaw **支持同一套技能格式**：你可以把 OpenClaw 的技能放到 `config/skills/` 下，助手就能用。我们保留自己的优势：**本地优先**、**RAG 记忆**、**多用户沙箱**，以及已经做好的**插件模型**。结果不是“克隆 OpenClaw”，而是“取两家之长”。

---

## HomeClaw 的现状

经过大量升级并改名为 **HomeClaw** 之后，系统目前是这样：

- **分布式部署：** Core 可以在一台机器、渠道在另一台，也可以在一台或多台电脑上跑多个 HomeClaw 实例。每个实例一个智能体，通过增加实例来扩展。

- **本地与云端 LLM：** 可以只用本地模型（llama.cpp、GGUF），只用云端（LiteLLM → OpenAI、Gemini、DeepSeek 等），或两者都用。主模型和嵌入模型可配置，运行时可切换。

- **内置与外部插件：** **内置插件**（Python）在 `plugins/` 目录——天气、新闻、邮件和你自己的。**外部插件**可以用**任意语言**（Node.js、Go、Java、Rust 等）编写，以独立 HTTP 服务运行；向 Core 注册并实现简单的请求/响应约定。所以你可以通过放入 Python 插件或接入已有服务来扩展。

- **OpenClaw 技能：** 可以复用 **OpenClaw 的技能**（`config/skills/` 下的 SKILL.md）。助手看到“可用技能”并用工具执行这些工作流。不必全部移植——只要能表达成“按这种方式用这些工具”，就可以在 HomeClaw 上跑。

- **多用户、RAG 记忆、权限：** 都还在。每个用户有隔离的对话与记忆；权限是一个简单配置文件；RAG 提供跨会话的长期、语义化回忆。

所以：**一个智能体**，但可以**分布式部署**，使用**本地与云端 LLM**，用**内置与外部插件**扩展，并使用 **OpenClaw 风格的技能**。现在是介绍它、邀请大家试用和参与建设的好时机。

---

## 你现在可以怎么用

你可以在自己的机器（或服务器）上**运行 HomeClaw**，通过 WebChat、CLI、Telegram、Discord、Slack、邮件、Matrix、微信、WhatsApp 或任何能向 `/inbound` API POST 的机器人与它**对话**。在 `config/user.yml` 里**配置**用户，在 `config/core.yml` 里**选择**本地和/或云端模型，按需**添加**插件与技能。助手会**记忆**（RAG + 对话历史）、**推理**（用你选的 LLM）、**做事**（通过工具与插件）。这些都可以只在你自己的硬件上完成，也可以在需要更强算力时混合使用云端。

它是**自托管**、**多用户**、**可扩展**的。文档（README、Design.md、Channel.md、HOW_TO_USE.md 以及 `docs/` 下的指南）会带你完成安装与扩展。

---

## 接下来：更好用，更聪明地用本地与云端

HomeClaw 现在就能用，但还有很多可改进。目前对我最重要的有两个方向。

**第一：让配置和运行简单得多。** 现在需要改 YAML、跑几条命令。我想要**更好的工具和界面**——向导、更清晰的默认值、稳定的 WebChat——让更多人不用通读手册就能把 HomeClaw 跑起来。目标是“几分钟把家庭助手上线”，而不是“先读设计文档”。

**第二：在不浪费钱的前提下用好本地与云端 LLM。** 理想情况是：**大部分事由本地模型做**，**只在真正需要时用云端**（更难的问题、更长上下文或用户明确要求）。这意味着更智能的路由、回退，或许还有简单策略（如“嵌入一律本地”“聊天：本地优先，置信度低时用云端”）。我想探索如何混合本地与云端，在**尽量节省成本**的同时仍提供良好体验。

所以故事还没结束。HomeClaw 会继续演进——更好用，更懂得何时用本地、何时用云端。如果你对一个本地优先、基于渠道、多用户的家庭助手感兴趣，希望你会试用、加星或贡献。你的家、你的 AI、你的掌控。

---

*感谢阅读。HomeClaw 开源（Apache 2.0）。仓库：[https://github.com/allenpeng0705/HomeClaw](https://github.com/allenpeng0705/HomeClaw)。*
