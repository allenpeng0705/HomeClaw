{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to HomeClaw","text":"<p>HomeClaw is a local-first AI assistant that runs on your own hardware. Talk to it over the channels you already use\u2014Telegram, Discord, WebChat, email, and more\u2014and extend it with plugins and skills. Your data stays yours.</p>"},{"location":"#get-started","title":"Get started","text":"Install Clone, pip install, optional local/cloud LLM setup Run Start Core and a channel (WebChat, Telegram, CLI, \u2026) Channels WhatsApp, Telegram, email, WebHook, and how to allow users Tools Built-in tools, plugins, and skills Models Local (llama.cpp) and cloud (LiteLLM) configuration Platform Config files, multi-user, memory backend, remote access Help Doctor, troubleshooting, and repo docs"},{"location":"#more","title":"More","text":"<ul> <li>Introducing HomeClaw \u2014 What it is and how it fits with OpenClaw  </li> <li>Story \u2014 Why and how HomeClaw was built  </li> </ul>"},{"location":"#one-sentence","title":"One sentence","text":"<p>HomeClaw is local- and cloud-LLM supported, multi-user, memory-based, and self-hosted. One agent, one memory, many channels\u2014your home, your AI, your control.</p>"},{"location":"channels/","title":"Channels","text":"<p>Channels are how you reach your HomeClaw assistant: WebChat, Telegram, Discord, email, CLI, and more. All use the same Core and the same permission list.</p>"},{"location":"channels/#supported-channels","title":"Supported channels","text":"Channel Type Run command CLI In-process <code>python -m main start</code> WebChat WebSocket <code>python -m channels.run webchat</code> Telegram Inbound <code>python -m channels.run telegram</code> Discord Inbound <code>python -m channels.run discord</code> Slack Inbound <code>python -m channels.run slack</code> Email Full Start email channel (see repo) Matrix, Tinode, WeChat, WhatsApp Full <code>python -m channels.run &lt;name&gt;</code> Webhook Relay <code>python -m channels.run webhook</code>"},{"location":"channels/#allow-who-can-talk","title":"Allow who can talk","text":"<p>Edit <code>config/user.yml</code>. Each user has <code>name</code>, <code>email</code>, <code>im</code> (e.g. <code>telegram_&lt;chat_id&gt;</code>, <code>discord_&lt;user_id&gt;</code>), and optional <code>permissions</code>. Only listed users can send messages to Core. See Multi-user support in the repo for details.</p>"},{"location":"channels/#inbound-api-any-bot","title":"Inbound API (any bot)","text":"<p>Any bot can POST to Core <code>/inbound</code> with:</p> <pre><code>{ \"user_id\": \"telegram_123456789\", \"text\": \"Hello\" }\n</code></pre> <p>Response: <code>{ \"text\": \"...\" }</code>. Add <code>user_id</code> to <code>config/user.yml</code>. If Core is not reachable from the internet, use the Webhook channel as a relay. See Channel.md and HowToWriteAChannel.md in the repo.</p>"},{"location":"getting-started/","title":"Getting started","text":"<p>Quick start for the doc site. You can replace with curated steps from README and HOW_TO_USE.md.</p> <ol> <li>Clone and install</li> </ol> <p><code>bash    git clone https://github.com/allenpeng0705/HomeClaw.git    cd HomeClaw    pip install -r requirements.txt</code></p> <ol> <li> <p>Configure \u2014 Edit <code>config/core.yml</code> (LLM, memory) and <code>config/user.yml</code> (who can talk to the assistant).</p> </li> <li> <p>Run Core \u2014 <code>python -m core.core</code> or <code>python -m main start</code></p> </li> <li> <p>Run a channel \u2014 e.g. <code>python -m channels.run webchat</code> and open http://localhost:8014</p> </li> </ol> <p>For full steps, see the main README and HOW_TO_USE.md in the repo.</p>"},{"location":"help/","title":"Help","text":"<p>Troubleshooting and where to find more.</p>"},{"location":"help/#check-config-and-llm","title":"Check config and LLM","text":"<pre><code>python -m main doctor\n</code></pre> <p>Runs checks and suggests fixes for config and LLM connectivity.</p>"},{"location":"help/#common-issues","title":"Common issues","text":"Issue What to try Permission denied Add your <code>user_id</code> (e.g. <code>telegram_&lt;chat_id&gt;</code>) to <code>config/user.yml</code> under <code>im</code> (or <code>email</code> / <code>phone</code>). Core not reachable Ensure Core is running (<code>python -m core.core</code> or <code>python -m main start</code>) and <code>channels/.env</code> has the correct <code>CORE_URL</code>. Channel connection error Start the channel process (e.g. <code>python -m channels.run webchat</code>). For full channels (Matrix, etc.), the channel must be running so Core can POST replies to <code>/get_response</code>. Web search \"unconfigured\" Set <code>TAVILY_API_KEY</code> (or another provider) in the environment or in <code>config/core.yml</code> under <code>tools.web.search</code>. Or use DuckDuckGo fallback (<code>fallback_no_key: true</code>). Browser tools fail Install Playwright browser: <code>python -m playwright install chromium</code>. Use the same Python env as Core."},{"location":"help/#docs-in-the-repo","title":"Docs in the repo","text":"<ul> <li>README.md \u2014 Overview, quick start, channels, plugins, skills.</li> <li>HOW_TO_USE.md \u2014 Step-by-step setup and usage.</li> <li>Design.md \u2014 Architecture and components.</li> <li>Channel.md \u2014 Channel usage and config.</li> <li>docs_design/ \u2014 PluginsGuide, SkillsGuide, MemoryAndDatabase, ToolsDesign, MultiUserSupport, RemoteAccess, etc.</li> </ul>"},{"location":"help/#github","title":"GitHub","text":"<ul> <li>Repo: https://github.com/allenpeng0705/HomeClaw</li> <li>Open an issue for bugs or feature requests.</li> </ul>"},{"location":"install/","title":"Install","text":"<p>HomeClaw runs on macOS, Windows, and Linux. You need Python 3.10\u20133.12 (recommended).</p>"},{"location":"install/#1-clone-and-install","title":"1. Clone and install","text":"<pre><code>git clone https://github.com/allenpeng0705/HomeClaw.git\ncd HomeClaw\npip install -r requirements.txt\n</code></pre> <p>For faster installs in China, you can use a mirror (e.g. <code>-i https://pypi.tuna.tsinghua.edu.cn/simple</code>).</p>"},{"location":"install/#2-optional-local-llm-llamacpp","title":"2. Optional: local LLM (llama.cpp)","text":"<p>To run local GGUF models, you need a llama.cpp server. The repo includes <code>llama.cpp-master/</code> with platform-specific binaries (e.g. <code>mac</code>, <code>linux_cpu</code>, <code>win_cpu</code>). Download GGUF model files (e.g. from Hugging Face) into a <code>models/</code> folder and configure <code>local_models</code> in <code>config/core.yml</code>. See Models for paths and ports.</p>"},{"location":"install/#3-optional-cloud-llm","title":"3. Optional: cloud LLM","text":"<p>For cloud models (OpenAI, Gemini, DeepSeek, etc.), set the API key as an environment variable (e.g. <code>OPENAI_API_KEY</code>, <code>GEMINI_API_KEY</code>) and add the model to <code>cloud_models</code> in <code>config/core.yml</code>. No extra install beyond <code>requirements.txt</code> (LiteLLM is included).</p>"},{"location":"install/#4-next-step","title":"4. Next step","text":"<p>After install, see Run to start Core and a channel. For full setup (config, users, memory), see the main HOW_TO_USE.md in the repo.</p>"},{"location":"introducing-homeclaw/","title":"Introducing HomeClaw","text":"<p>Formal intro for the doc site. You can replace this with curated content from <code>docs_design/IntroducingHomeClaw.md</code> or write a shorter version.</p> <p>HomeClaw is a local-first AI assistant that runs on your machine. One installation is one agent: the same memory, tools, and plugins no matter how you connect (WebChat, Telegram, email, CLI, etc.).</p> <ul> <li>Local-first \u2014 Use local LLMs (llama.cpp, GGUF) so data stays at home; cloud (LiteLLM) is optional.</li> <li>Channels \u2014 Reach your assistant from Telegram, Discord, WeChat, WhatsApp, email, and more.</li> <li>Memory \u2014 RAG-based, per-user; the assistant can recall across sessions.</li> <li>Plugins &amp; skills \u2014 Extend with plugins (Weather, News, Mail, custom) and OpenClaw-style skills.</li> </ul> <p>For the full intro, see IntroducingHomeClaw.md in the repo.</p>"},{"location":"models/","title":"Models","text":"<p>HomeClaw supports local LLMs (llama.cpp, GGUF) and cloud LLMs (OpenAI, Gemini, DeepSeek, etc. via LiteLLM). You can use one or both; main and embedding model are configured separately.</p>"},{"location":"models/#local-models","title":"Local models","text":"<ul> <li>Run GGUF models via a llama.cpp server. Place model files in a <code>models/</code> directory (or path set by <code>model_path</code> in <code>config/core.yml</code>).</li> <li>In <code>config/core.yml</code>, under <code>local_models</code>, add entries with <code>id</code>, <code>path</code> (relative to <code>model_path</code>), <code>host</code>, <code>port</code>. Set <code>main_llm</code> and <code>embedding_llm</code> to e.g. <code>local_models/&lt;id&gt;</code>.</li> <li>Start the llama.cpp server(s) for each model (see <code>llama.cpp-master/README.md</code> in the repo). You can use the bundled binaries in <code>llama.cpp-master/&lt;platform&gt;/</code>.</li> </ul>"},{"location":"models/#cloud-models","title":"Cloud models","text":"<ul> <li>In <code>config/core.yml</code>, under <code>cloud_models</code>, add entries with <code>id</code>, <code>path</code> (LiteLLM model name, e.g. <code>openai/gpt-4o</code>), <code>host</code>, <code>port</code>, and <code>api_key_name</code> (e.g. <code>OPENAI_API_KEY</code>).</li> <li>Set the environment variable with that name where Core runs (e.g. <code>export OPENAI_API_KEY=...</code>). Do not put API keys in the config file.</li> <li>Set <code>main_llm</code> or <code>embedding_llm</code> to e.g. <code>cloud_models/OpenAI-GPT4o</code>.</li> </ul> <p>Supported providers include OpenAI, Google Gemini, DeepSeek, Anthropic, Groq, Mistral, xAI, OpenRouter, and more. See LiteLLM docs.</p>"},{"location":"models/#mix-local-and-cloud","title":"Mix local and cloud","text":"<p>You can use a local model for chat and a cloud model for embedding (or the other way around). Set <code>main_llm</code> and <code>embedding_llm</code> to the appropriate <code>local_models/&lt;id&gt;</code> or <code>cloud_models/&lt;id&gt;</code>. Switch at runtime via CLI: <code>llm set</code> (local) or <code>llm cloud</code> (cloud), or by editing <code>config/core.yml</code> and restarting Core.</p>"},{"location":"platform/","title":"Platform","text":"<p>Config, deployment, and multi-user behavior.</p>"},{"location":"platform/#configuration-files","title":"Configuration files","text":"File Purpose <code>config/core.yml</code> Core host/port, <code>main_llm</code>, <code>embedding_llm</code>, <code>local_models</code>, <code>cloud_models</code>, <code>use_memory</code>, <code>use_tools</code>, <code>use_skills</code>, memory backend (Cognee/Chroma), <code>tools.*</code>, auth, etc. <code>config/user.yml</code> Allowlist of users: <code>name</code>, <code>id</code>, <code>email</code>, <code>im</code>, <code>phone</code>, <code>permissions</code>. Required for channels. <code>channels/.env</code> <code>CORE_URL</code> (e.g. <code>http://127.0.0.1:9000</code>), bot tokens (e.g. <code>TELEGRAM_BOT_TOKEN</code>). <code>config/email_account.yml</code> IMAP/SMTP for the email channel (if used)."},{"location":"platform/#multi-user","title":"Multi-user","text":"<p>Chat history, memory, and profile are keyed by system user id (from <code>config/user.yml</code>). Each user has isolated data. Add users under <code>users:</code> in <code>config/user.yml</code> with the correct <code>im</code> / <code>email</code> / <code>phone</code> for the channel. See MultiUserSupport.md in the repo.</p>"},{"location":"platform/#memory-backend","title":"Memory backend","text":"<ul> <li>Default: Cognee (SQLite + Chroma + Kuzu by default; configurable to Postgres, Qdrant, Neo4j via Cognee).</li> <li>Alternative: Chroma (in-house): set <code>memory_backend: chroma</code> in <code>config/core.yml</code>; configure <code>database</code>, <code>vectorDB</code>, <code>graphDB</code> there. See MemoryAndDatabase.md in the repo.</li> </ul>"},{"location":"platform/#remote-access-and-auth","title":"Remote access and auth","text":"<p>If you expose Core on the internet, set <code>auth_enabled: true</code> and <code>auth_api_key</code> in <code>config/core.yml</code>. Clients must send <code>X-API-Key</code> or <code>Authorization: Bearer &lt;key&gt;</code> on <code>/inbound</code> and <code>/ws</code>. See RemoteAccess.md in the repo. Use Tailscale or SSH for secure access when needed.</p>"},{"location":"plugins/","title":"Plugins","text":"<p>How to extend HomeClaw with plugins. Replace with curated content from docs_design/PluginsGuide.md.</p> <p>Plugins add focused capabilities: weather, news, email, custom APIs. One plugin = one feature.</p> <ul> <li>Built-in (Python) \u2014 In <code>plugins/</code> with <code>plugin.yaml</code>, <code>config.yml</code>, <code>plugin.py</code>. Core discovers them at startup.</li> <li>External (any language) \u2014 Run as an HTTP server; register with Core via <code>POST /api/plugins/register</code>.</li> </ul> <p>See PluginsGuide.md and HowToWriteAPlugin.md in the repo.</p>"},{"location":"run/","title":"Run","text":"<p>How to start HomeClaw Core and channels.</p>"},{"location":"run/#1-start-core","title":"1. Start Core","text":"<p>From the project root:</p> <pre><code>python -m core.core\n</code></pre> <p>Or run the interactive CLI (Core runs in a background thread; you chat in the terminal):</p> <pre><code>python -m main start\n</code></pre> <p>Core listens on port 9000 by default (<code>config/core.yml</code>).</p>"},{"location":"run/#2-start-a-channel","title":"2. Start a channel","text":"<p>In another terminal, start a channel so you can talk to the assistant:</p> Channel Command Typical URL / use WebChat <code>python -m channels.run webchat</code> http://localhost:8014 Telegram <code>python -m channels.run telegram</code> Your Telegram bot Discord <code>python -m channels.run discord</code> Your Discord bot CLI (use <code>python -m main start</code> above) Terminal only <p>Set <code>channels/.env</code> with <code>CORE_URL</code> (e.g. <code>http://127.0.0.1:9000</code>) and any bot tokens. Add allowed users in <code>config/user.yml</code>.</p>"},{"location":"run/#3-quick-test","title":"3. Quick test","text":"<ol> <li>Start Core: <code>python -m core.core</code> or <code>python -m main start</code>.</li> <li>Start WebChat: <code>python -m channels.run webchat</code>.</li> <li>Open http://localhost:8014 and send a message. Ensure your <code>user_id</code> is in <code>config/user.yml</code>.</li> </ol>"},{"location":"run/#4-doctor-check-config-and-llm","title":"4. Doctor (check config and LLM)","text":"<pre><code>python -m main doctor\n</code></pre> <p>Checks config and LLM connectivity and suggests fixes. See Help for troubleshooting.</p>"},{"location":"story/","title":"Why and how HomeClaw was built","text":"<p>Story-style intro. You can replace with curated content from docs_design/HomeClaw_story.md.</p> <p>HomeClaw started as GPT4People: local LLM for security, channels to reach your home computer simply. It grew to include RAG memory, major IMs (WeChat, WhatsApp), then cloud model support, a plugin framework, multi-user and multi-model design, and learning from OpenClaw (skills, ideas) while keeping local-first and privacy.</p> <p>For the full story, see HomeClaw_story.md in the repo.</p>"},{"location":"tools/","title":"Tools","text":"<p>HomeClaw provides tools the LLM can call by name (file, exec, browser, cron, memory, web search, sessions, etc.) and plugins for focused features (Weather, News, Mail). Enable with <code>use_tools: true</code> in <code>config/core.yml</code>.</p>"},{"location":"tools/#tool-categories","title":"Tool categories","text":"Category Examples Files / folders <code>file_read</code>, <code>file_write</code>, <code>file_edit</code>, <code>folder_list</code>, <code>document_read</code> Web <code>fetch_url</code>, <code>web_search</code>, <code>browser_navigate</code>, <code>browser_snapshot</code>, <code>browser_click</code> Memory <code>memory_search</code>, <code>memory_get</code> (when use_memory) Scheduling <code>cron_schedule</code>, <code>cron_list</code>, <code>remind_me</code>, <code>record_date</code> Sessions <code>sessions_list</code>, <code>sessions_transcript</code>, <code>sessions_send</code>, <code>sessions_spawn</code> Routing <code>route_to_plugin</code>, <code>route_to_tam</code>, <code>run_skill</code> <p>Config (allowlists, timeouts, API keys) is under <code>tools:</code> in <code>config/core.yml</code>. See ToolsDesign.md and ToolsAndSkillsTesting.md in the repo.</p>"},{"location":"tools/#plugins","title":"Plugins","text":"<p>Plugins add single-feature capabilities (weather, news, email). The LLM routes to them via <code>route_to_plugin(plugin_id)</code>.</p> <ul> <li>Built-in (Python): In <code>plugins/</code> with <code>plugin.yaml</code>, <code>config.yml</code>, <code>plugin.py</code>.</li> <li>External (any language): HTTP server; register with Core via <code>POST /api/plugins/register</code>.</li> </ul> <p>See PluginsGuide.md and HowToWriteAPlugin.md in the repo.</p>"},{"location":"tools/#skills","title":"Skills","text":"<p>Skills (SKILL.md under <code>config/skills/</code>) describe workflows; the LLM uses tools to accomplish them or calls <code>run_skill</code> to run a script. OpenClaw-style skills can be reused. See SkillsGuide.md and ToolsSkillsPlugins.md in the repo.</p>"}]}